$subject$=Математическая статистика
$teacher$=Лекции Блаженова А. В.
$date$=06.05.2025

\section{Лекция 13.}

\subsection{Нормализация регрессионного анализа}

Пусть имеется уравнение общей линейной регрессии $\vec X = Z^T \vec \beta + \vec \varepsilon$, где $n$ - число экспериментов, $\vec X$ - столбец результатов экспериментов, $Z$ - матрица плана, $\vec \beta$ - столбец коэффициентов регрессии, $\vec \varepsilon$ - вектор теоретических ошибок

При этом ранее предполагали, что выполнены условия:

\begin{enumerate}
    \item $\operatorname{Cond. 1}$: Строки $Z$ - независимы
    \item $\operatorname{Cond. 2}$: $\varepsilon_i \in N(0, \sigma^2)$ и независимы
\end{enumerate}

Условие 2 часто нарушается

\subsubsection{Взвешенный метод наименьших квадратов}

Пусть $\varepsilon_i \in N(0, v_i \sigma^2)$ и независимы (то есть дисперсия ошибки зависит от номера наблюдения). Другими словами, $D \vec \varepsilon = \sigma^2 V$, где $V = \operatorname{diag} (v_1, \dots, v_n)$

Логично наблюдениям с меньшей дисперсии ошибки предать больший вес. Пусть вес $w_i = \frac{1}{v_i}$. Домножим обе части уравнения регрессии на $\sqrt{w_i}$, тогда получим $\vec \tilde{X} = \tilde Z^T \vec \beta + \vec \tilde{\varepsilon}$, где $\tilde x_i=  \sqrt{w_i} x_i$, $\tilde Z^{(j)}_i = \sqrt{w_i} Z^{(j)}_i$, $\tilde \varepsilon_i = \sqrt{w_i} \varepsilon_i$

$D \vec \tilde{\varepsilon} = D(\sqrt{w_i} \varepsilon_i) = w_i D \varepsilon_i = \frac{1}{v_i} v_i \sigma^2 = \sigma^2$ - получаем, что $D \vec \tilde{\varepsilon} = \sigma^2 E_n$, то есть стандартную ситуацию

Тогда оценки $\vec b$ будут несмещенными и эффективными

Недостаток у этого метода: нужно знать коэффициенты $v_i$

\Ex Рассмотрим модель линейной парной регрессии без свободного члена $X = \beta_0 Z + \varepsilon$

Теоретическое уравнение $A \vec B = Z \vec X$, где $Z = \left(Z_1, \dots, Z_n\right)$ - матрица плана, $\vec B = \hat \beta_0$, $A = Z Z^T = z_1^2 + \dots + z_n^2$, $Z \vec X = z_1 x_1 + \dots + z_n x_n$

$\sum_{i = 1}^n z^2_i \hat \beta_0 = \sum_{i = 1}^n z_i x_i \Longrightarrow \hat \beta_0 = \frac{\sum_{z_i x_i}}{\sum z^2_i}$ - оценка МНК 

По взвешенному методу наименьших квадратов $\tilde \beta_0 = \frac{\sum w_i z_i x_i}{\sum w_i z_i^2}$ - оценка взвешенного МНК

\ExN{a} Взвешенное среднее

Допустим, что проводим серию измерений \enquote{скоропортящимся} инструментом. При $Z \equiv 1$: $X = \beta_0 + \varepsilon$, $\varepsilon_i \in N(0, v_i \sigma^2), w_i = \frac{1}{v_i}$

Тогда $\hat \beta_0 = \frac{\sum_{i = 1}^n w_i x_i}{\sum_{i = 1}^n w_i}$ - взвешенное среднее

\ExN{б} Повторное наблюдение

Пусть было $n$ серий по $k_i$ наблюдений ($1 \leq i \leq n$). В каждой серии вычислили выборочное среднее $\overline{x_i}$. 
Если $\varepsilon \in N(0, \sigma^2)$, то дисперсия ошибки для каждого выборочного среднего $D\varepsilon_i = \frac{\sigma^2}{k_i}$. Оценка по результатам всех наблюдений будет $\hat \beta_0 = \frac{\sum_{i = 1}^n k_i \overline{x_i}}{\sum_{i = 1}^n k_i}$

\ExN{в} Пропорция

Пусть $X$ - потери тепла в квартире. Основной фактор $Z$ - разница температур снаружи и внутри. Так как при $Z = 0$ $X = 0$, то уравнением регрессии будет $X = \beta_0 Z + \varepsilon$

Логично предположить, что дисперсия ошибки зависит от величины $Z$. Рассмотрим две гипотезы:

\begin{enumerate}
    \item Дисперсия ошибки прямо пропорциональна $Z$: $D\varepsilon_i = C Z_i = \sigma^2 \frac{C Z_i}{\sigma^2}$

    Тогда $w_i = \frac{\sigma^2}{C Z_i}$ и $\hat \beta_0 = \frac{\sum_{i = 1}^n \frac{\sigma^2}{C Z_i} Z_i X_i}{\sum_{i = 1}^n \frac{\sigma^2}{C Z_i} Z_i^2} = \frac{\sum X_i}{\sum Z_i} = \frac{\overline{x}}{\overline{z}}$

    \item Дисперсия ошибки квадратично зависит от $Z$: $D\varepsilon_i = C Z_i^2 = \sigma^2 \frac{C Z_i^2}{\sigma^2}$

    Тогда $w_i = \frac{\sigma^2}{C Z_i^2}$ и $\hat \beta_0 = \frac{\sum_{i = 1}^n \frac{\sigma^2}{C Z_i^2} Z_i X_i}{\sum_{i = 1}^n \frac{\sigma^2}{C Z_i^2} Z_i^2} = \frac{\sum X_i}{\sum Z_i} = \frac{\sum \frac{X_i}{Z_i}}{n} = \overline{\left(\frac{x}{z}\right)}$
\end{enumerate}

\subsection{Коррелированные наблюдения}

Пусть ошибки не только имеют различные дисперсии, но и коррелированны между собой: $\cov (\varepsilon_i, \varepsilon_j) = v_{ij}$

Тогда $D \vec \varepsilon = \sigma^2 V$, где $V = \left(v_{ij}\right)$

Так как матрица ковариаций симметричная и положительно определенная, то существует $\sqrt{V}$. Домножим обе части уравнения регрессии на $\sqrt{V^{-1}}$:

$\vec X = Z^T \vec{\beta} + \vec \varepsilon \ \Big| \ \cdot \sqrt{V^{-1}}$

$\vec \tilde{X} = \tilde Z^T \vec \beta + \varepsilon$, где $\vec \tilde{X} = \sqrt{V^{-1}} \vec X$, $\tilde Z = \sqrt{V^{-1}} Z$, $\vec \tilde{\varepsilon} = \sqrt{V^{-1}} \vec \varepsilon$

Тогда матрица ковариаций нового вектора ошибок будет $D \vec \tilde{\varepsilon} = D(\sqrt{V^{-1}} \vec \varepsilon) = \sqrt{V^{-1}} D \vec \varepsilon \left(\sqrt{V^{-1}}\right)^T = \sigma^2 I_n$

То есть получили классическую ситуацию, когда выполнено $\operatorname{Cond. 2}$ и вектор оценок $\hat \beta_0$ будет несмещенным и эффективным

\subsection{Составление матрицы плана при управляемом эксперименте}

Если строки матрицы плана взять ортогональными, то дисперсии оценки коэффициентов $b_i$ регрессии будут минимальными. Поэтому лучше матрицу плана составлять таким образом:

Дисперсии оценок при этом $D b_i = \sigma^2 A^{-1}_{ii}$. Если $Z$ - ортогональная (не обязательно нормированная), то $A = Z Z^T = E_n$, а $D b_i = \frac{\sigma^2}{Z_i^2}$. Несложно доказать, что во всех других случаях дисперсия будет больше

\subsection{Метод главных осей}

Помимо метода наименьших квадратов существует метод \enquote{главных осей}. Идея следующая: матрицу ковариаций приводим к диагональной форме

В МНК мы минимизируем расстояние отрезков, параллельных оси Oy, а в методе главных осей - перпендикуляр от точки до возможной прямой

Результатом метода главных осей получаем прямую, являющуюся главной осью эллипса, появляющегося из корреляционного облака

\subsection{Нелинейные регрессии}

Помимо общего МНК многие нелинейные зависимости могут быть сведены к линейным при помощи простых приемов

\ExN{а} $X = \alpha + \beta f(Z) + \varepsilon$, где $f(x)$ - известная функция

Тогда можно взять новый фактор $Z^\prime = f(Z)$, свели задачу к стандартной, получаем уравнение $X = \alpha + \beta Z^\prime + \varepsilon$

Пример: $X = \alpha + \beta \ln Z + \varepsilon$, то $Z^\prime = \ln Z$

\ExN{б} $X = \alpha Z^\beta \varepsilon$

Логарифмируем: $\ln X = \ln \alpha + \beta \ln Z + \ln \varepsilon \Longleftrightarrow X^\prime = \alpha^\prime + \beta Z^\prime + \varepsilon^\prime$

\ExN{в} $X = \alpha e^{\beta Z} \varepsilon$

Логарифмируем: $\ln X = \ln \alpha + \beta Z + \ln \varepsilon \Longleftrightarrow X^\prime = \alpha^\prime + \beta Z + \varepsilon^\prime$

\ExN{г} Зависимость в виде полинома: $X = \beta_0 + \beta_1 Z + \beta_2 Z^2 + \dots + \beta_k Z^k$

Введем новые факторы $Z_1 = Z, Z_2 = Z^2, \dots, Z_k = Z^k$

$X = \beta_0 + \beta_1 Z + \beta_2 Z_2 + \dots + \beta_k Z_k$

При этом, чтобы избежать мультиколлинеарность, лучше брать $k < 4$. При больших $k$ получить многочлен большой степени, который сможет гарантировано пройти через все точки - это будет статистически незначимо


\Nota Если из теории мы знаем вид зависимости и подбираем ее под данные, то желательно строить модель как можно проще

\Notas Из построенных моделей предпочтительней та, где коэффициент детерминации больше

\mediumvspace

Построение даже удачной регрессионной модели не означает появление причинно-следственной связи. Исторический пример: исследовалась точность бомбометания от различных условий. Пусть $X$ - точность, $Z_1$ - высота, $Z_2$ - ветер, $Z_3$ - количество истребителей противника

Построили модель $X = \beta_0 + \beta_1 Z_1 + \beta_2 Z_2 + \beta_3 Z_3$ и получили $\hat \beta_3 < 0$ - то есть при большем числе техники противника точность увеличивалась. Оказалось, что не был учтен фактор облачности - при нем $\hat \beta_3 > 0$ и коэффициент детерминации улучшился

Или другой пример: корреляция численности аистов и рождения детей в Голландии XX века оказалась прямой






