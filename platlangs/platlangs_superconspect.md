# <a name="%D0%BF%D0%BB%D0%B0%D1%82%D1%84%D0%BE%D1%80%D0%BC%D1%8B-%D0%B8-%D1%81%D1%80%D0%B5%D0%B4%D1%8B-%D0%B2%D1%8B%D0%BF%D0%BE%D0%BB%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F-%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2-%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F"></a> Платформы и среды выполнения языков программирования


* [Платформы и среды выполнения языков программирования](#%D0%BF%D0%BB%D0%B0%D1%82%D1%84%D0%BE%D1%80%D0%BC%D1%8B-%D0%B8-%D1%81%D1%80%D0%B5%D0%B4%D1%8B-%D0%B2%D1%8B%D0%BF%D0%BE%D0%BB%D0%BD%D0%B5%D0%BD%D0%B8%D1%8F-%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2-%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F)
  * [Лекция 1. Введение в компиляторы и виртуальные машины](#%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-1.-%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B2-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80%D1%8B-%D0%B8-%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D1%8B)
  * [Лекция 2. История, цели и сложности](#%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-2.-%D0%B8%D1%81%D1%82%D0%BE%D1%80%D0%B8%D1%8F%2C-%D1%86%D0%B5%D0%BB%D0%B8-%D0%B8-%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D0%B8)
  * [Лекция 3. Требования и решения](#%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-3.-%D1%82%D1%80%D0%B5%D0%B1%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F-%D0%B8-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F)
  * [Лекция 4. Конвейер компиляции на примере JavaScript](#%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-4.-%D0%BA%D0%BE%D0%BD%D0%B2%D0%B5%D0%B9%D0%B5%D1%80-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%86%D0%B8%D0%B8-%D0%BD%D0%B0-%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D0%B5-javascript)
  * [Лекция 5. Компилятор в виртуальной машине](#%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-5.-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80-%D0%B2-%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%B5)
  * [Лекция 6. Анализ и оптимизации компилятора](#%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-6.-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%B8-%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80%D0%B0)
    * [Оптимизации](#%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8)
    * [Анализ кода](#%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%BA%D0%BE%D0%B4%D0%B0)
  * [Лекция 7. Компилятор в виртуальной машине](#%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-7.-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80-%D0%B2-%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%B5)
    * [Анализ времени жизни](#%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%B8-%D0%B6%D0%B8%D0%B7%D0%BD%D0%B8)
    * [Аллокация регистров](#%D0%B0%D0%BB%D0%BB%D0%BE%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-%D1%80%D0%B5%D0%B3%D0%B8%D1%81%D1%82%D1%80%D0%BE%D0%B2)
    * [Генерация кода](#%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D1%8F-%D0%BA%D0%BE%D0%B4%D0%B0)



На курсе будут рассматриваться

* дизайн языков программирования, какие сложности возникают
* как запускать свои языки поверх виртуальной машины
* как компилировать языки программирования, какие оптимизации применяются
* что такое интерпретатор, байт-код
* как работает система типов, сборщик мусора
* чем отличаются JIT-компиляция и AOT-компиляция

## <a name="%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-1.-%D0%B2%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5-%D0%B2-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80%D1%8B-%D0%B8-%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D1%8B"></a> Лекция 1. Введение в компиляторы и виртуальные машины

<!-- Лектор - Гаврин Е. А. -->

Разработку ПО можно разделить на две части:

* разработка прикладного ПО: офисное ПО, игры, научное ПО, финансовое ПО и так далее
* разработка системного ПО: базы данных, операционные системы, компилятор и так далее

Существенной проблемой при разработке является трудность управления памятью в так называемых неуправляемых языках (например, C и C++). Поэтому появились

* языки с безопасной работой с памятью (memory-safe), которые обнаруживают проблемы на этапе компиляции, такие как Rust
* санитайзеры - инструменты для обнаружения проблем с памятью и не только во время выполнения программы, которые используют специально инструментированную компиляцию

Позже в 90-ых начали появляться управляемые языки: Java, C#, JavaScript. Как следствие появились виртуальные машины, которые управляли памятью и исполняли байт-код

Теперь условно можно выделить два типа языков:

* со статическим компилятором, как C, C++
* с интерпретатором или виртуальной машиной, как Java, C#

Каждый язык соответствовал одной из этих целей:

* язык, программы на котором работают быстро, а время компиляции и запуска нам неважно
* язык, где не нужна пиковая производительность, но важен быстрый запуск, например, JavaScript


В компиляторе берется исходный код, он представляется в виде промежуточных структур, таких как абстрактное синтаксическое дерево (Abstract Syntax Tree, AST) и граф потока управления (Control Flow Graph, CFG), а потом превращается в нативный для процессора/ОС код, который исполняется процессором

Термин AOT-компиляция (Ahead-of-Time) предполагает, что код обрабатывается до непосредственного выполнения. Напротив, JIT-компиляция (Just-in-Time) - это компиляция байт-кода в машинный код прямо во время выполнения программы. Это позволяет применять оптимизации на основе данных о реальном выполнении программы (профилирование)

В случае с виртуальной машиной исходный код превращается в промежуточный код, которой после исполняется виртуальной машиной

---

За последние два десятка лет очень изменилось то, как мы используем наши устройства - сейчас мы работаем с большой комбинацией устройств, и это создает множество совместных сценариев работы с устройствами

В индустрии стремятся сделать так, чтобы приложения разрабатывались под разные устройства

Так как бизнес хочет наилучшее решение за наименьшие деньги и время, нужно, чтобы устройства имели одну кодовую базу. Для запуска приложения на разных устройствах нужны виртуальные машины, фреймворки и библиотеки, реализованные для каждой из целевых платформ

Оказывается, что количество выручки с продаж потребительской электроники растет, а люди ценят наличие экосистемы среди своих устройств

Из-за возникшей дуополии Google и Apple это кажется простой задачей, однако существует огромное количество производителей, которые имеют свои маркеты приложений (около 400). С пользовательской точки зрения, когда на рынке 400 маркетов, становится больно пользоваться экосистемами

Появилась альтернатива - миниприложения. Такие миниприложения, написанные на JavaScript, встроены в мессенджер WeChat. Оказалось, что миниприложения начали преобладать над этими 400 маркетами, так как они были намного удобнее

Для разработчика мобильных приложений это означает:

* разработать две версии приложения: одно для Android, одно для iOS
* загрузить как минимум в несколько магазинов приложений: для России - это как минимум 3 (Google Play, AppStore, RuStore), для Китая - 8
* поддерживать версии для разных платформ и соблюдать все правила и процедуры разных маркетов

Маленькому бизнесу становится дорого это поддерживать, поэтому нам нужен инструмент, который позволит решить эту проблему более эффективно. Проще всего сделать одну виртуальную машину под разные устройства

Многие производители телефонов в первую очередь зарабатывают с продажи аппаратного обеспечения. Они имеют широкую линейку продуктов и данные использования от реальных пользователей. 

Разработчикам приложений важно минимизировать усилия на разработку приложений, поэтому для их привлечения нужна знакомая экосистема разработки и простой доступ к заработку

А пользователям важно, чтобы приложения были быстрыми, а батарея держалась долго, поэтому нужно, чтобы приложения были мгновенными, энергоэффективными и работали внутри экосистемы устройств

Не всегда разработка кроссплатформенных решений облегчает создание новых продуктов, например Oracle с агрессивным лицензированием Java и появившийся в ответ на это C#. Потребность в большинстве языках программирования появилась с ростом рынка потребительской электроники, такие как Go, Swift, Kotlin, Dart. Например, Kotlin появился как ответ на медленные обновления языка Java. Swift - как современная альтернатива Objective-C. React - как попытка сделать кроссплатформенное решение для JavaScript

Кроссплатформенные решения поддерживаются ограниченным набором компании (Google, Apple, Microsoft и так далее). Такие компании заинтересованы, потому что они получают основную выгоду

<!-- Первое, что выбирается при разработке приложения - фреймворк для интерфейса. Современный фреймворк для интерфейса должен поддерживать множество платформ, а также фреймворк должен быть основан на кроссплатформенном языке программирования, обеспечивая консистентность среди устройств -->

Разберем примеры кроссплатформенных решений:

* Платформа .NET и фреймворк Xamarin как единая кодовая база для разработки

* Для Android виртуальная машина Dalvik (позже Android Runtime, ART). Изначально Dalvik был простым интерпретатором. Позже в него добавили JIT- и AOT-компиляцию. Далее сборка профилей использования кода позволила компилировать приложения на серверной инфраструктуре

* Для браузеров есть виртуальная машина (например, V8 для JavaScript), пользовательский интерфейс и движок для рендера

* LLVM - платформа для построения компиляторов. Она предоставляет универсальное промежуточное представление (Intermediate Representation, IR), на котором проводятся оптимизации, и бэкенды для генерации машинного кода под разные архитектуры. Например, на основе LLVM разработан компилятор Clang для C/C++



## <a name="%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-2.-%D0%B8%D1%81%D1%82%D0%BE%D1%80%D0%B8%D1%8F%2C-%D1%86%D0%B5%D0%BB%D0%B8-%D0%B8-%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D0%B8"></a> Лекция 2. История, цели и сложности

<!-- Лектор - Недоря А. Е. -->

Разработка языков программирования началась с создания языка для машины ЭНИАК (ENIAC, Electronic Numerical Integrator and Computer)

Программирование на ЭНИАК осуществлялось соединением нужных разъемов на панели проводами и зачастую занимало недели

ЭНИАК использовалась для расчета вычисления, связанных с ядерным оружием, прогнозирования погоды, инженерных расчетов

Первым широкоиспользуемым языком стал FORTRAN, созданный в 1950-ых Джоном Бэкусом в IBM. Далее в 1956-1958 создается LISP при участии Джона Маккарти

Большинство популярных языков, таких как Go, Rust, Typescript, Kotlin, используемых в индустрии, были разработаны в течение нескольких лет (4-6 лет), что подтверждает, что создание промышленного языка программирования - задача нетривиальная


Целями создания языка программирования могут быть разными. Например, 

* C# был создан, чтобы избежать юридических проблем
* Swift и Kotlin как замена устаревших языков Objective-C и Java
* Go и Rust как удовлетворение высоких требований по безопасности и производительности

Поэтому у конкретного языка может быть ограниченная область применения

Так что же нужно для создания языка:

* Документация языка, а именно спецификация, описание грамматики, документация стандартной библиотеки, туториалы
* Тесты для проверки компилятора соответствии спецификации
* Компилятор, умеющий генерировать код для разных архитектур и ОС
* Стандартная библиотека
* Поддержка выполнения кода (управление памятью и параллелизмом)
* Среда разработки, инструменты для дебага
* JIT-компилятор
* Пакетный менеджер
* Модели памяти, параллелизма
* Непротиворечивость системы типов, гарантии (например, что не бывает утечки памяти)

Разработка языка программирования - огромная работа, которая может не прекращаться. Разработка начинается с формулировки требований и приоритетов:

* Важнее скорость программы или продуктивность разработчика?
* Важнее надежность и безопасность или другие характеристики?
* Каков будет способ управления и виды параллелизма?
* Каков будет синтаксис, статическая и динамическая семантики?

Также нужно выбрать:

* компромисс между эффективностью, продуктивностью, безопасностью и переносимостью
* эволюцию (что-то лучшее) или революцию
* динамическая система типов или статическая
* управление памятью: ручное, сборщик мусора, подсчет ссылок, владение и заимствование
* управление параллелизмом через потоки или корутины
* экосистема: создать новую или приспособиться к существующей

Что бы разработчик не выбрал, кому-то это можно не понравится. Нельзя уверенно сказать, какая комбинация решений приведет к наилучшему исходу без прототипирования, а функции языка могут взаимодействовать друг с другом

Пример взаимодействия функций языка из языка Go:

```go
package main

import "fmt"

type IM interface {
    Method()
}

type SM struct {i int}

fune (x *SM) Method() {
    x.i = 1
}

func check(im IM) {
    if im != nil {
        im.Method()
    }
    fmt.Println("checked")
}
    
func main() {
    check(nil) // checked

    var S_nil *SM = nil
    check(S_nil) // segfault
}
```

Здесь функция `check` правильно обработает значение `nil`, однако при передачи указателя на объект `SM` интерфейса `IM`, равного `nil`, выбрасывается ошибка доступа к адресу, так как по адресу `nil` нет объекта с методом `Method()`


## <a name="%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-3.-%D1%82%D1%80%D0%B5%D0%B1%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F-%D0%B8-%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F"></a> Лекция 3. Требования и решения

<!-- Лектор - Недоря А. Е. -->
  
Современная система программирования или приложение содержит компоненты, написанные на нескольких языках, так как ни один язык не удовлетворяет все потребности. Части системы, такие как бизнес-логика, фреймворки, UI, требуют разных языков, которые отличаются в управлении памятью, параллелизмом, системе типом и так далее

Тогда появилась идея создания семейства языков, которые будут хорошо вместе взаимодействовать, вместо языка, который будет встраиваться с другими

Тогда для этих языков нужно писать компилятор, значит, нужно выбрать язык, на котором нужно написать этот компилятор

При этом, в нашем семействе должен быть язык, на котором будет приятно писать следующие версии нашего компилятора, чтобы снизить зависимость и влияние от сторонних языков

В начале этот первый язык должен быть легко читаемым, легко понимаемым, иметь все нужные простые функции и механизмы и ничего того, что выходит за нашу область. Также в нем должно быть уменьшено число ошибок времени исполнения, безопасность null-типов, системы типов, сборщик мусора и модульность

Производительность первого языка не должна быть приоритетной для нас

Название языка должно быть простым для понимания. После этого можно начать предварительный дизайн: дизайн грамматики и система типов

Шаг второй - разработка компилятора на другом языке и исполнения скомпилированного кода. Далее компилятор пишется на самом языке, что позволяет протестировать наш язык в прикладной задаче

Грамматика и синтаксис языка должна быть простой, а семантика - очевидной, не должно быть скрытой семантики и "злой магии"

Среди базовых типов можно выбрать байт (8-битное целое число), 64-битное целое число, 64-битное вещественное число, логический тип, символ, строка. Для создания компилятора могут также понадобиться вектор (динамический массив), опциональный тип (вместо null) и классы 

Другие продвинутые механизмы могут вызвать дополнительные трудности в разработке:

* Цикл `foreach` может потребовать другие функции, такие как итераторы и кортежи
* ООП добавляет дополнительно семантики для конструкторов, а безопасность сложно достичь
* Шаблонные типы добавляют сложности в семантике и тяжело реализуемы



## <a name="%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-4.-%D0%BA%D0%BE%D0%BD%D0%B2%D0%B5%D0%B9%D0%B5%D1%80-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%86%D0%B8%D0%B8-%D0%BD%D0%B0-%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80%D0%B5-javascript"></a> Лекция 4. Конвейер компиляции на примере JavaScript

<!-- Лектор - Гаврин Е. А. -->

Сейчас большинство веб-приложений написаны на JavaScript - динамически типизируемом языке для скриптов с похожим на язык C синтаксисом

Код на JavaScript могут исполнять множество движков (виртуальных машин)

Хотя в JavaScript есть примитивные типы, многие операции заставляют их вести себя как объекты через автоматическую упаковку в объекты. У объекта в JavaScript есть свойства, у свойства - имя и значения, а сами свойства могут добавляться и удаляться во время исполнения

В JavaScript вместо наследование использованы прототипы: каждый объект хранит в себе ссылку на прототип другого объекта, тем самым наследуя его свойства. Объекты могут представлять собой цепочку прототипов, которую можно изменять во время исполнения

Конвертация типов в JavaScript позволяют выполнять такие операции:

```js
var x = 1 + 2;  // 3
x = 1 + "2";    // "12"
x = 1 + {};     // "1[object Object]"
x = +"23";      // 23
```

Из-за прототипов, свойств и динамической типизации JavaScript очень динамический, что представляет трудность для реализации виртуальной машины. Из-за этого появляются проблемы в разработке: приходящий тип объекта не соответствует ожидаемому

Далее появился TypeScript, позволяющий аннотировать типы и проверять их перед исполнением программы

Движок JavaScript работает так:

* Исходный код через компилятор превращается в промежуточный код
* Далее интерпретатор исполняет промежуточный код вместе со внешними библиотеками, представленными заранее в виде промежуточного кода

Спецификация JavaScript не указывает формат промежуточного кода, поэтому разработчик движка волен сам выбрать его формат. Интерпретатор также содержит сборщик мусора, который управляет памятью, и совершает оптимизации над кодом

Внутри компилятора:

* Парсер читает исходный код и представляет его в виде абстрактного синтаксического дерева (Abstract Syntax Tree, AST)
* Далее это дерево анализируется и представляется в виде байткода (промежуточного кода)

После этого интерпретатор исполняет байткод

Из-за того, что JavaScript - динамически типизированный, выражение `x + y` может представлять из себя:

* Чтение операции
* Чтение указателей операндов
* Проверка типов
* Распаковка объектов в значения
* **Вычисление операции нужным способом исходя из типов**
* Запись результата в память
* Упаковка значений в объекты

Из-за этого JavaScript обладает низкой производительностью

Главная цель для JavaScript - мгновенно исполнять код, чтобы веб-страницы загружались быстро. Поэтому парсер и интерпретатор не должны тратить ресурсы на оптимизации. Другой путь - компиляция после парсера, что замедляет время запуска

Сейчас в браузерах байткод после парсера сразу исполняется интерпретатором, а в другом потоке горячие регионы - часто вызывающиеся участки кода - компилируются и исполняются нативно

Всего можно выделить 4 уровня:

1. Уровень 0, интерпретатор - нет оптимизации
2. Уровень 1, базовые частичные оптимизации без сбора профилей
3. Уровень 2, базовые частичные оптимизации со сбором профилей
4. Уровень 3, полностью оптимизированная JIT-компиляция

Если кусок кода вызывается больше какого-то выбранного числа, то далее выбирается нужные оптимизации исходя из информации о выполнении кода

Большая часть веб-приложений и мобильных приложений используют такую гибридную модель: часть кода исполняется в интерпретаторе, часть кода нативно как машинные инструкции


Старая версия движка JavaScriptCore использовала три уровня исполнения

* LLInt (Low Level Interpreter) - интерпретация на низком уровне
* Baseline JIT - базовые оптимизации
* DFG JIT (Data Flow Graph JIT) - JIT-компиляция на основе графа потока данных

В новой версии добавился новый уровень:

* FTL JIT (Fourth Tier LLVM JIT) - JIT-компиляция с помощью LLVM

Позднее поняли, что с таким конвейером тяжело работать, поэтому упростили инфраструктуру

--- 

Интерпретатор имеет ряд достоинств

* независимость от платформы
* рефлексия - изменение кода во время исполнения
* простота в разработке

И недостатков: большое потребление памяти и низкая скорость по сравнению с машинным кодом

Интерпретацию можно ускорить с помощью AOT- и JIT-компиляций, скрытых классов (структур, с помощью которых можно быстрее обрабатывать сложные объекты). Также можно применить такие оптимизации:

* Для определения операции из инструкции байткода не следует использовать блоки if/switch. Вместо этого можно использовать:

    * Прямое перенаправление (Direct Threading) - изменение указателя на текущую инструкцию на значение, равное коду инструкции (opcode)

    * Непрямое перенаправление (Indirect Threading) - переход делается не напрямую по коду инструкции, а через уровень косвенной адресации (сначала берётся адрес в таблице, затем по нему происходит переход)

    * Перенаправление токенов (Token Threading) - использование токенов-указателей


* Встроенное кеширование (Inline Caching)

    Вместо того, чтобы заново проверять типы для исполнения операции, можно для часто применяющихся ситуаций (например, для сложения двух чисел) перенаправлять на другой поток инструкций

    Встроенное кеширование имеет несколько состояний

    * Неинициализированное - нет записей в кеше
    * Мономорфное - один тип объекта
    * Полиморфное - 2-4 типа объектов
    * Мегаморфное - много типов, используется медленный путь


## <a name="%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-5.-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80-%D0%B2-%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%B5"></a> Лекция 5. Компилятор в виртуальной машине

<!-- Лектор - Большов Максим -->

Интерпретатор проще реализовать, чем компилятор. Он исполняет исходный код построчно, без предварительного преобразования в машинный код

Главное преимущество - портируемость: один и тот же исходный код может выполняться на разных устройствах, если для каждой платформы есть интерпретатор

Недостаток интерпретации — низкая скорость выполнения, поскольку анализ и исполнение происходят во время работы программы

Чтобы ускорить выполнение, придумали JIT-компиляцию (Just-In-Time) - гибридный подход, который совмещает интерпретацию и компиляцию

JIT-компилятор во время исполнения анализирует программу и определяет, какие участки кода выполняются чаще всего (так называемые "горячие" участки)

Эти участки компилируются в машинный код на лету, чтобы повысить производительность. При этом редко исполняемые части кода остаются интерпретируемыми, что экономит время и память.

Примеры сред, использующих JIT:

* Android Runtime (ART) - используется в Android вместо старого Dalvik VM
* V8 - движок JavaScript от Google, применяемый в Chromium и Node.js
* ArkTS VM (HarmonyOS) - виртуальная машина Huawei, поддерживающая JIT и AOT

---

Процесс компиляции делится на три основные стадии:

1. Фронтенд представляет из себя:

    * Лексический анализ (tokenization)
    * Синтаксический анализ (парсинг, построение AST)
    * Семантический анализ (проверка типов, области видимости и так далее)

2. Миддленд:

    * Преобразует код в промежуточное представление (IR, Intermediate Representation)
    * Выполняет оптимизации, не зависящие от платформы (например, удаление мертвого кода, свертка констант, упрощение выражений)

3. Бекенд:

    * Преобразует IR в машинный код для конкретной архитектуры
    * Применяет платформозависимые оптимизации (например, размещение регистров, инструкции SIMD)


Промежуточное представление - это форма кода, удобная для анализа и преобразования.
Хорошее промежуточное представление обладает следующими свойствами:

* Простое для генерации и анализа
* Независимое от платформы
* Эффективное для последующей трансляции в машинный код
* Достаточно богатое, чтобы выражать сложные конструкции языка

Примеры IR:

* LLVM IR в экосистеме LLVM
* Three-address code (TAC) - простая линейная форма
* SSA (Static Single Assignment) - форма, где каждая переменная присваивается ровно один раз

SSA - особое представление промежуточного кода, где каждая переменная имеет только одно место присваивания.
Если в коде встречаются ветвления, то значения объединяются через специальные фи-функции (φ-functions)

Преимущества SSA: упрощение анализа потока данных, повышение эффективности оптимизаций, облегчение удаления ненужных переменных


## <a name="%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-6.-%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%B8-%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80%D0%B0"></a> Лекция 6. Анализ и оптимизации компилятора

<!-- Лектор - Большов Максим -->
<!-- Презентация - Черных Сергей, chernykh.sergey1@huawei.com -->

### <a name="%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8"></a> Оптимизации

Оптимизации компилятора - это преобразования программы, которые делают её быстрее, компактнее или экономичнее, не изменяя поведение. Оптимизации компилятора бывают:

1. Локальные - действуют внутри одного базового блока, например, свертка констант, удаление мертвого кода
2. Глобальные - анализируют всю функцию или граф потока управления (CFG), например, устранение общих подвыражений, перемещение инвариантов циклов
3. Межфункциональные (interprocedural) - охватывают несколько функций, например, inlining (встраивание), link-time optimization (оптимизация на этапе компоновки)

Приведем несколько простых оптимизаций:


* Глазок (Peephole) - локальная оптимизация, рассматривающая маленькие фрагменты кода (2–5 инструкций) и заменяющая их на более эффективные эквиваленты. Например, такой код:

    ```c++
    int a = b + 3;
    int d = a + 5;

    int c = d * 2;
    ```

    заменяется на:

    
    ```c++
    int d = b + 5;

    int c = d * 2;
    ```

* Схлопывание (или свертка) констант (Constant Folding) - вычисление выражений на этапе компиляции

    ```c++
    int x = 2 * 3; // заменяется на int x = 6;
    ```

* Удаление мертвого кода (Dead Code Elimination) - удаление кода, результат которого не используется, а также кода после return или неиспользуемых переменных

* Устранение общих подвыражений (Common Subexpression Elimination) - устранение общих подвыражений, таким образом, их вычисление производится один раз. Например,

    ```c++
    a = ((b - c) % d) * ((b - c) / e);
    ```

    превращается в:

    ```c++
    f = (b - c);
    a = (f % d) * (f / e);
    ```

* Встраивание (Inlining) - подстановка тела функции вместо вызова. Например:

    ```c++
    int square(int x) { 
        return x * x; 
    }
    int y = square(5); // заменяется на int y = 5 * 5;
    ```

* Loop Invariant Code Motion - вынос из цикла выражений, не зависящих от итерации. Например,

    ```c++
    for (int i = 0; i < N; i++) {
        int x = N * N;
        int j = x * i;
    }
    ```

    превращается в:

    ```c++
    int x = N * N;
    for (int i = 0; i < N; i++) {
        int j = x * i;
    }
    ```

* Induction Variable Elimination - устранение или объединение переменных, изменяющихся линейно с номером итерации. Например,

    ```c++
    for (int i = 0; i < N; i++) {
        j = 2 * i + 1;
        A[j] = 0;
    }
    ```

    превращается в:

    ```c++
    for (int j = 1; j < 2 * N; j += 2) A[j] = 0;
    ```


* Раскрутка цикла (Loop Peeling) - отделение первых итераций цикла для оптимизации граничных условий. Например,

    ```c++
    for (int i = 0; i < N; i++) A[i] = f(i);
    ```

    превращается в:

    ```c++
    A[0] = f(0);
    for (int i = 1; i < N; i++) A[i] = f(i);
    ```

* Loop Unrolling (развёртывание цикла) - выполнение нескольких итераций за одну, чтобы уменьшить количество сравнений

    ```c++
    for (int i = 0; i < N; i++) A[i]++;
    ```

    превращается в:

    ```c++
    for (int i = 0; i < N; i += 4) {
        A[i]++; A[i+1]++; A[i+2]++; A[i+3]++;
    }
    ```

* Loop Fission - один цикл на два для улучшения кэш-локальности или параллелизма. Например,

    ```c++
    for (int i = 0; i < N; i++) {
        A[i] = f(i);
        B[i] = g(A[i]);
    }
    ```

    превращается в:

    ```c++
    for (int i = 0; i < N; i++) A[i] = f(i);
    for (int i = 0; i < N; i++) B[i] = g(A[i]);
    ```

* Loop Fusion - слияние два цикла, проходящих по тем же данным. Например,

    ```c++
    for (int i = 0; i < N; i++) A[i] = B[i] + 1;
    for (int i = 0; i < N; i++) C[i] = A[i] * 2;
    ```

    превращается в:

    ```c++
    for (int i = 0; i < N; i++) {
        A[i] = B[i] + 1;
        C[i] = A[i] * 2;
    }
    ```

### <a name="%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%BA%D0%BE%D0%B4%D0%B0"></a> Анализ кода

Для большинства оптимизаций используется анализ граф управления потоком (Control Flow Graph, CFG) программы. Для него применяются обходы графа:

* DFS (Depth-First Search) - поиск в глубину, используется для построения деревьев и обнаружения циклов
* RPO (Reverse Post Order) - порядок обхода, при котором каждый узел посещается после своих предков, удобен для итеративных алгоритмов анализа данных

---

Блок `V` доминирует над блоком `W`, если любой путь от входа в функцию до `W` проходит через `V`. Блок `V` называют доминатором. Начальный блок доминирует над всеми

Дерево доминаторов строится при помощи DFS и служит для оптимизаций, анализа циклов и построения SSA-формы

---

Цикл в графе потока управления определяется через наличие обратного ребра (back-edge) - перехода из блока `B` в блок `A`, где `A` доминирует над `B`.

Основные понятия:

* Back-edge - ребро, указывающее назад в цикл
* Header - первый блок цикла (точка входа), доминирует над всеми блоками цикла
* Latch - блок, из которого выполняется переход обратно к Header
* Preheader - блок перед циклом, используется для вставки кода, выполняющегося один раз перед входом в цикл
* Exiting edge - ребро, по которому управление выходит из цикла
* Exit block - блок, куда программа попадает после выхода из цикла
* Entering block (или Loop predecessor) - блок, ведущий в Header
* Irreducible loop - цикл с несколькими входами, который не может быть представлен единственным Header
* Countable loop - цикл с известным числом итераций, например `for (int i = 0; i < N; i++)`

Алгоритм анализа циклов обычно выполняется так:

1. Строится граф управления потока функции
2. Для каждого ребра `B -> A` проверяется, доминирует ли `A` над `B`. Если да, то это обратное ребро
3. Из найденных обратных рёбер выделяются множества блоков, формирующих циклы
4. Для каждого цикла определяются header, latch, preheader и выходы
5. Строится иерархия вложенности циклов (дерево цикла), что помогает выполнять оптимизации, такие как loop unrolling, fusion, invariant motion и другие


## <a name="%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D1%8F-7.-%D0%BA%D0%BE%D0%BC%D0%BF%D0%B8%D0%BB%D1%8F%D1%82%D0%BE%D1%80-%D0%B2-%D0%B2%D0%B8%D1%80%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B9-%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%B5"></a> Лекция 7. Компилятор в виртуальной машине

<!-- Лектор - Большов Максим -->
<!-- Презентация - Сидоров Алексей -->

Когда виртуальная машина компилирует программу, она не преобразует её напрямую из байткода или AST в машинный код. Реализация современных JIT/AOT-компиляторов устроена иначе: они используют промежуточное представление программы - IR (Intermediate Representation). Промежуточное представление - это набор инструкций, графов, блоков, который достаточно далёк от конкретных особенностей аппаратной платформы, но уже достаточно формален для того, чтобы над ним выполнять оптимизации

Промежуточное представление надо превратить в реальный машинный код - набор инструкций процессора с конкретной архитектурой (arm, x86, RISC-V и других). Программа, которая переводит промежуточное представление к конкретному бинарному коду называется бекендом компилятора

Бекенд состоит из нескольких ключевых фаз, каждая из которых решает строго определённую задачу. Чтобы понимать, как компилятор действительно порождает код, нужно разбираться в логике этих фаз:

1. Анализ времени жизни (Liveness Analysis) - определение жизненных диапазонов значений
2. Аллокация регистров (Register Allocation) - выбор, какие значения будут жить в регистрах, а какие - в памяти
3. Генерация кода (Code Generation) - превращение IR в реальный машинный код с учётом всех особенностей архитектуры


Высокоуровневое промежуточное представление - это представление программы, свободное от особенностей конкретной машины. Однако на практике даже промежуточное представление создаётся с оглядкой на целевую платформу, например, для упрощенного перевода инструкций под основную архитектуру

Высокоуровневое промежуточное представление:

* позволяет использовать структуру SSA (Static Single Assignment)
* позволяет легко анализировать зависимости
* содержит гораздо больше логических значений, чем существует аппаратных регистров
* может моделировать операции, которым нет прямого аналога в процессоре

И именно здесь возникает фундаментальная проблема: в промежуточном представлении значений может быть сотни, а аппаратных регистров - меньше тридцати, например, ARM64 предоставляет 31 регистр общего назначения

В промежуточном представлении легко создать 300 временных значений, но в реальном процессоре такое невозможно

Компилятору требуется понять:

* какие значения живут одновременно
* какие значения можно переместить в один и тот же физический регистр в разные моменты
* когда значение можно выбросить, потому что оно больше нигде не используется
* какие значения необходимо временно выгружать в память, если регистров не хватает

### <a name="%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7-%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%B8-%D0%B6%D0%B8%D0%B7%D0%BD%D0%B8"></a> Анализ времени жизни

Если некоторая инструкция создала значение A, то это значение имеет время жизни с момента определения до последнего использования. Но, так как программа состоит из блоков, между которыми есть переходы, циклы, ветвления, то нельзя просто определить, в какой момент значение A больше не нужно; нужно понять:

* где значение может понадобиться
* проходят ли пути управления через эти места
* каким образом значения живут в циклах

Это приводит к важному определению: значение A живо в точке P, если существует путь от P до какого-либо использования A

Не важно, реальный ли это путь или потенциальный, поэтому говорят, что статическая живость - это приближение реального поведения программы

Настоящий жизненный цикл значения на конкретном запуске может быть короче, но статический анализ обязан учитывать худший случай. Отсюда возникают понятий:

* Отрезок времени жизни (Live Range) - это отрезок линейной нумерации инструкций, где значение живо
* Интервал времени жизни (Live Interval) - конкретный интервал между определением и последним использованием
* Дыра времени жизни (Live Hole) - участок, где значение временно не живо, но общая форма интервала его включает

---

Чтобы анализ живости был возможен, нужно упорядочить базовые блоки графа потока управления в линейную последовательность.

Это не просто список блоков, а порядок, в котором:

* доминирующие блоки идут до доминируемых
* блоки циклов группируются
* внешние предшественники учитываются корректно
* нерегулярные циклы тоже обходятся, хотя несколько иначе

Алгоритм расчёта живости работает с конца в начало, а регистровый аллокатор типа линейного сканирования ожидает, что программа развёрнута в линейную последовательность инструкций. Линейная нумерация - это своего рода временная шкала, вдоль которой мы будем измерять жизнь значений

Далее анализ живости выполняется в несколько фаз:

1. Обход графа потока управления идёт в обратном линейном порядке блоков

    Для каждого блока вычисляется набор живых значений (live-set) - множество значений, которые должны быть живы в начале блока.

    Набор живых значений блока складывается из:

    1. объединения наборов живых значений всех блоков-последователей
    2. всех реальных аргументов фи-функции этих последователей

    Вход фи-функции выбирается в зависимости от того, из какого блока пришёл контроль, поэтому значение, необходимое фи-функции, должно быть живо в конце блока-предшественника его ветви

    После определения набора живых значений блока каждая инструкция из набора получает живой диапазон, включающий весь блок: `[block.begin, block.end)`

2. Теперь внутри блока мы анализируем инструкции, начиная с конца:

    1. Если инструкция определяет значение, то интервал времени жизни этого значения должен начинаться здесь - это минимальная точка его жизни
    1. Мы исключаем эту инструкцию из набора живых значений, потому что её значение уже учтено
    2. Каждый операнд инструкции добавляется в набор живых значений (если он не был там)
    3. Для каждого такого значения расширяется его отрезок времени жизни: оно должно быть живо как минимум до точки использования

    Здесь впервые начинает образовываться структура живых интервалов: значения, которые используются позже, живут дольше; значения, которые не используются, постепенно покидают набор живых значений

    После полного анализа всех инструкций блока необходимо удалить значения, которые участвуют в фи-инструкциях в этом же блоке, поскольку их жизнь фактически начинается в начале блока, а не продолжается в его конце


3. Если блок является заголовком цикла, то все значения, находящиеся в наборе живых значений в начале блока, обязаны перекрывать весь цикл. Это означает, что отрезок времени жизни расширяется до `[loop_header.begin, loop_end)`.

    Если значение живо во входе цикла и живо в одном из его тел, оно должно быть непрерывно живым для всей кольцевой структуры, чтобы не возникло разрывов, нарушающих моделирование циклического контроля

    Этот шаг обеспечивает целостность данных в циклах для любого алгоритма распределения регистров

### <a name="%D0%B0%D0%BB%D0%BB%D0%BE%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-%D1%80%D0%B5%D0%B3%D0%B8%D1%81%D1%82%D1%80%D0%BE%D0%B2"></a> Аллокация регистров

Теперь, когда мы знаем, где каждое значение живёт, можно распределять регистры. 

У процессора мало регистров. Например:

* ARM64 имеет 31 универсальный регистр
* x86_64 имеет 16 универсальных регистров, но при этом многие зарезервированы: sp, fp, lr, zero-register

Если бы регистров было 200–300, аллокация регистров не была бы нужна, но добавление регистров дорого само по себе:

* растёт площадь кристалла
* растёт энергопотребление
* усложняются машинные инструкции (увеличивается длина кодирования)
* возрастает нагрузка на схемы буфера и арифметико-логический блок

Основная цель аллокатора регистров - минимизировать обращения к памяти, то есть вытеснений значений с регистров (spill) и внесений значений (fill)

Тут есть несколько подходов, но самым простым является линейное сканирование (Linear Scan). Линейное сканирование:

* работает очень быстро (почти линейно)
* способен работать непосредственно с интервалами времени жизни
* довольно просто реализуется
* подходит для JIT, где время компиляции критично

Простая версия линейного сканирования выглядит так:

1. Интервалы значений сортируются по точке начала
2. Ведётся список активных интервалов - тех, которые сейчас живы
3. Когда начинается новый интервал:

    * Если есть свободный регистр - выдаём его
    * Если регистров нет - необходимо вытеснить одно из активных значений

Самый простой способ выбрать, кого вытеснять - выбрать интервал с максимально дальним концом. Но это приводит к ужасному количеству вытеснений и внесений, потому что алгоритм не смотрит на ближайшее использование значения

Лучше выбирать интервал с самой далёкой следующей точкой использования. Это позволяет:

* не вытеснять значение, которое требуется прямо сейчас
* переносить вытеснение на менее критические отрезки времени

Ещё одна важная оптимизация: не нужно вытеснять весь интервал целиком, можно разбить его на части. Это радикально уменьшает количество вытеснение и внесение и часто спасает производительность

Выбор стратегии вытеснения должен быть правильным. Если алгоритм выбрал неудачное значение для вытеснения, начинается эффект домино:

* его операнды приходится каждый раз загружать со стека
* чтобы конструировать инструкции, нужны временные регистры
* регистров становится не хватать ещё больше
* начинается лавинообразный рост вытеснений и внесений

### <a name="%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D1%8F-%D0%BA%D0%BE%D0%B4%D0%B0"></a> Генерация кода

Генерация кода - последняя стадия бекенда. Она использует ранее полученные артефакты:

* интервалы времени жизни
* распределённые регистры
* стековые слоты
* линейную нумерацию блоков
* инструкции промежуточного представления в удобной форме

Генерация кода работает так:

1. Формируются пролог и эпилог: выделяется стековый фрейм и так далее
2. Выделяются временные регистры, которые не участвуют в регаллоке
3. Генерируется код блоков в линейном порядке, вставляя необходимые переходы и метки
4. Инструкции промежуточного представления преобразуются в инструкции целевой архитектуры, учитывая:

    * тип операндов (регистр, стековый слот)
    * особенности архитектуры (ограниченные иммедиаты и так далее)
    * соглашение о вызовах

5. Создаётся метаинформация для виртуальной машины и сборщика мусора:

    * карты живых значений
    * адреса инструкций
    * таблицы для обработки исключений
    * таблицы переходов к медленным путям, где виртуальная машина вмешивается

Стековый фрейм (или фрейм вызова) содержит:

* параметры, переданные через стек
* слоты для вытеснения для аллокатора регистров
* вспомогательные слоты для вызовов из кода
* служебные данные для сборщика мусора

Особенно важно то, что адресация к слотам происходит через регистры sp или fp, поэтому генератор кода обязан аккуратно работать с этими регистрами

---

Однако, вместо бекенда можно использовать LLVM () - проект программной инфраструктуры для создания компиляторов, однако есть ключевые проблемы:

1. Она слишком тяжёлая для JIT-компиляции: огромные структуры данных, много стадий оптимизации
2. Нельзя менять основу промежуточного представления: если виртуальная машина использует собственную модель промежуточного представления, то нужно подстраиваться под представление LLVM, что дорого и неудобно
3. Промежуточное представление LLVM IR имеет строгие инварианты, которые JIT-компилятор нарушать не хочет
4. Сильная зависимость от внешнего проекта: для критически важного элемента виртуальной машины это опасно
5. Размер кода и время компиляции с LLVM существенно больше, чем у лёгких аллокаторов регистров вроде линейного сканирования
6. В AOT-компиляции LLVM подходит отлично, но в JIT-компиляции - слишком тяжеловесен



