\clearpage

\section{X. Программа экзамена в 2024/2025}

\begin{enumerate}
    \item \textit{Пространство элементарных исходов. Случайные события. Операции над событиями.}

    \hyperlink{spaceofelementaryoutcomes}{Пространство элементарных исходов}: Пространством элементарных исходов $\Omega$ называется множество, содержащее все возможные исходы
    экспериментов, из которых при испытании происходит ровно один. Элементы этого множества называются
    элементарными исходами и обозначаются $\omega$

    \hyperlink{randomeventdefinition}{Случайное событие}: Случайными событиями называется подмножество $A \subset \Omega$. События $A$ наступают, если произошел один из
    элементарных исходов из множества $A$

    \hyperlink{randomeventoperations}{Операции над событиями}: Суммой $A + B$ называется событие, состоящее в том, что произошло события $A$ или событие $B$ (хотя бы одно из них)

    Произведением $A \cdot B$ называется событие, состоящее в том, что произошло событие $A$ и событие $B$ (оба из них)

    Противоположным $A$ событием называется событие $\overline{A}$, состоящее в том, что событие $A$ не произошло

    Дополнение (разность) $A \setminus B$ называется событие $A \cdot \overline{B}$

    События $A$ и $B$ называются несовместными, если их произведение - пустое множество
    (не могут произойти одновременно при одной эксперименте)

    События $A$ влечет события $B$, если $A \subset B$ (если наступает $A$, то наступит $B$)

    \item \textit{Статистическое определение вероятности. Классическое определение вероятности.}

    \hyperlink{statisticaldefinitionofprobability}{Статистическое определение вероятности}: Пусть проводится $n$ реальных экспериментов, при которых событие $A$ появилось $n_A$ раз.
    Отношение $\frac{n_A}{n}$ называется частотой события $A$.
    Эксперименты показывают, что при увеличении числа $n$ частота стабилизируется у некоторого числа,
    при котором мы понимаем статистическую вероятность: $P(A) \approx \frac{n_A}{n}$ при $n \to \infty$

    \hyperlink{classicdefinitionofprobability}{Классическое определение вероятности}: Пусть пространство случайных событий $\Omega$ содержит конечное число равновозможных исходов,
    тогда применимо классическое определение вероятности: \fbox{$P(A) = \frac{|A|}{|\Omega|} = \frac{m}{n}$}, где $n$ - число всех возможных исходов, $m$ - число благоприятных исходов

    \item \textit{Геометрическое определение вероятности. Задача Бюффона об игле.}

    \hyperlink{geometricdefinitionofprobability}{Геометрическое определение вероятности}: Пусть $\Omega \subset \Real^n$ - замкнутая ограниченная область,
    $\mu(\Omega)$ - мера $\Omega$ в $\Real^n$ (например, длина отрезка, площадь области на плоскости, объем тела в пространстве), в этом случае применимо геометрическое определение вероятности: $P(A) = \frac{\mu(A)}{\mu(\Omega)}$

    \hyperlink{buffonsproblem}{Задача Бюффона об игле}: пусть пол вымощен ламинатом, $2l$ - ширина доски, на пол бросается игла длины, равной ширине доски,
    найти вероятность того, что игла пересечет стык доски

    Определим положение иглы координатами центра и углом, между иглой и стыком доски, причем можно считать, что эти величины независимы

    $\letsymbol x \in [0; l]$ - расстояние от центра до ближайшего края, $\varphi \in [0; \pi]$ - угол

    $\Omega = [0; l] \times [0; \pi]$

    Событие $A$ (пересечет стык) наступает, если $x \leq l \sin \varphi$

    $P(A) = \frac{S(A)}{S(\Omega)} = \frac{\int_0^\pi l \sin \varphi d \varphi}{\pi l} = \frac{2l}{\pi l} = \frac{2}{\pi}$

    \item \textit{Аксиоматическое определение вероятности. Вероятностное пространство. Свойства вероятности.}

    \hyperlink{axiomaticdefinitionofprobability}{Аксиоматическое определение вероятности}: $\letsymbol\ \Omega$ - пространство элементарных исходов, $\mathcal{F}$ - его $\sigma$-алгебра событий.
    \textit{Вероятностью} на $(\Omega, \mathcal{F})$ называется функция $P: \mathcal{F} \to \Real$ со свойствами:

    \begin{enumerate}
        \item $P(A) \geq 0 \quad \forall A \in \mathcal{F}$ (неотрицательность)

        \item Если $A_1, A_2, \dots, A_n, \dots \in \mathcal{F}$ - несовместное, то $P(\sum_{i = 1}^\infty A_i) = \sum_{i = 1}^\infty P(A_i)$ (свойство счетной аддитивности)

        \item $P(\Omega) = 1$ (условие нормированности)
    \end{enumerate}

    \hyperlink{probabilityspace}{Вероятностное пространство}: Вероятностное пространство - тройка $(\Omega, \mathcal{F}, P)$

    \hyperlink{probabilityproperties}{Свойства вероятности}: 

    \begin{enumerate}
        \item Так как $\emptyset$ и $\Omega$ - несовместные, то $1 = P(\Omega) = P(\Omega + \emptyset) = 1 + P(\emptyset) \Longrightarrow P(\emptyset) = 0$

        \item Формула обратной вероятности: $P(A) = 1 - P(\overline{A})$

        \item $P(A) = 1 - P(\overline{A}) \leq 1$
    \end{enumerate}

    \item \textit{Аксиома непрерывности. Ее смысл и вывод.}

    \hyperlink{continuityaxiom}{Аксиома непрерывности}: \Ths Пусть имеется убывающая цепочка событий $A_1 \supset A_2 \supset A_3 \supset \dots \supset A_n \supset \dots$ и $\bigcap_{i = 1}^\infty A_n = \emptyset$

    Тогда $P(A_n) \underset{n \to \infty}{\to} 0$

    При непрерывном изменении области $A \subset \Omega \subset \Real^n$ соответствующая вероятность $P(A)$ также должна изменятся непрерывно

    \begin{MyProof}
        Ясно, что $A_n = \sum_{i = n}^\infty A_i \overline{A}_{i + 1} + \prod_{i = n}^\infty A_i$

        $\prod_{i = n}^\infty A_i = A_n \cdot \prod_{i = n + 1}^\infty A_i = \prod_{i = 1}^n
        \cdot \prod_{i = n + 1}^\infty A_i = \prod_{i = 1}^\infty = \emptyset \Longrightarrow
        A_n = \sum_{i = n}^\infty A_n \overline{A_{n + 1}}$ и так как эти события несовместны,
        то по свойству счетной аддитивности $P(A_n) = \sum_{i = n}^\infty P(A_i \overline{A_{i + 1}})$ - это остаток (хвост) сходящегося ряда

        $P(A_1) = \sum_{i = 1}^\infty P(A_i \overline{A_{i + 1}}) = \sum_{i = 1}^{n - 1} P(A_i \overline{A_{i + 1}}) + P(A_n)$ и $P(A_n) \underset{n \to \infty}{\to} 0$ по необходимому признаку сходимости
    \end{MyProof}

    \item \textit{Свойства операций сложения и умножения. Формула сложения вероятностей.}

    \hyperlink{probabilityoperationsproperties}{Свойства операций сложения и умножения}:

    \begin{enumerate}
        \item Свойство дистрибутивности: $A \cdot (B + C) = AB + AC$

        \item Формула сложения: если $A$ и $B$ несовместны, то $P(A + B) = P(A) + P(B)$

        \item Формула сложения вероятностей: $P(A + B) = P(A) + P(B) - P(AB)$
    \end{enumerate}

    \item \textit{Независимость событий. Независимые события в совокупности и попарно. Пример Бернштейна. }

    \hyperlink{independantevents}{Независимые события}: События $A$ и $B$ называются независимыми, если $P(A \cdot B) = P(A) \cdot P(B)$

    События $A_1, A_2, \dots A_n$ - независимы в совокупности, если для любого набора $i_1, i_2, \dots, i_k \ (2 \leq k \leq n)$
    $P(A_{i_1} \cdot A_{i_2} \cdot \dots \cdot A_{i_k}) = P(A_{i_1}) \cdot P(A_{i_2}) \cdot \dots \cdot P(A_{i_k})$

    \hyperlink{bernshteinsexample}{Пример Бернштейна}: Пусть имеется правильный тетраэдр, одна грань окрашена в красный, вторая в синий, третья в зеленый, а четвертая во все эти три цвета.

    Подбросили тетраэдр, $\letsymbol A$ - грань, которая содержит красный цвет, $B$ - синий, $C$ - зеленый.

    $P(A) = P(B) = P(C) = \frac{2}{4} = \frac{1}{2}$

    Так как $P(AB) = P(AC) = P(BC) = \frac{1}{4}$

    $P(AB) = \frac{1}{4} = \frac{1}{2} \cdot \frac{1}{2} = P(A) P(B)$ - попарная независимость

    $P(ABC) = \frac{1}{4} \neq P(A) P(B) P(C)$ - но вот независимость в совокупности не соблюдается

    \item \textit{Условная вероятность. Формула умножения событий.}

    \hyperlink{conditionalprobability}{Условная вероятность} $P(A|B)$ (или $P_B(A)$) - вероятность события $A$, вычисленная в предположении, что событие $B$ уже произошло. $P(A|B) = \frac{P(AB)}{P(B)}$

    \hyperlink{eventsmultiplicationformula}{Формула умножения событий}:

    Для двух событий: $P(AB) = P(B) \cdot P(A|B) = P(A) \cdot P(B|A)$

    В общем случае: $P(A_1 A_2 A_3 \dots A_n) = P(A_1) P(A_2 | A_1) P(A_3 | A_1 A_2) \dots P(A_n | A_1 A_2 \dots A_{n - 1})$

    \item \textit{Полная группа событий. Формула полной вероятности. Формула Байеса.}

    \hyperlink{completegroupofevents}{Полная группа событий}: События $H_1, H_2, \dots, H_n, \dots$ образуют полную группу событий, если они попарно несовместны и содержат все возможные элементарные исходы

    \hyperlink{formulaofcompleteprobability}{Формула полной вероятности}: $\letsymbol H_1, H_2, \dots, H_n, \dots$ - полная группа событий. Тогда $P(A) = \sum_{i = 1}^\infty P(H_i) P(A | H_i)$
    
    \hyperlink{bayesformula}{Формула Байеса}: $\letsymbol H_1, H_2, \dots, H_n$ - полная группа событий, и известно, что событие $A$ уже произошло

    Тогда $P(H_k | A) = \frac{P(H_k) P(A | H_k)}{\sum_{i = 1}^\infty P(H_i) P(A | H_i)}$

    \item \textit{Последовательность независимых испытаний. Формула Бернулли. Наиболее вероятное число успехов в схеме Бернулли.}

    \hyperlink{bernoullischema}{Схемой Бернулли} называется серия одинаковых независимых экспериментов, каждый из которых имеет 2 исхода: произошло интересующее нас событие или нет

    \hyperlink{bernoulliformula}{Формула Бернулли}: Вероятность того, что при $n$ испытаниях произойдет ровно $k$ успехов, равна
    $p_n(k) = C_n^k p^k q^{n - k}$

    \hyperlink{themostprobablenumberofsuccesses}{Наиболее вероятное число успехов}: 

    \begin{enumerate}
        \item $np$ - целое, тогда $np + p$ - нецелое, и $k = np$ - наиболее вероятное

        \item $np + p$ - нецелое, тогда $k = \lfloor np + p \rfloor$

        \item $np + p$ - целое, тогда $np + p - 1$ - целое, тогда $k \in \{np + p - 1, np + p\}$
    \end{enumerate}

    \item \textit{Локальная и интегральная формулы Муавра-Лапласа (без док-ва).}

    \hyperlink{localformulademoivrelaplace}{Локальная формула}: $p_n(k) \underset{n \to \infty}{\longrightarrow} \frac{1}{\sqrt{npq}} \varphi(x)$, где $\varphi(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ - функция Гаусса, 
    $x = \frac{k - np}{\sqrt{npq}}$

    \hyperlink{integralformulademoivrelaplace}{Интегральная формула}: $p_n(k_1 \leq k \leq k_2) \underset{n \to \infty}{\longrightarrow} \Phi(x_2) - \Phi(x_1)$, где $\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_0^x e^{-\frac{z^2}{2}} dz$ - функция Лапласа,
    $x_1 = \frac{k_1 - np}{\sqrt{npq}}$ - отклонение от левой границы, $x_2 = \frac{k_2 - np}{\sqrt{npq}}$ - отклонение от правой

    \item \textit{Вероятность отклонения относительной частоты от вероятности события. Закон больших чисел Бернулли.}

    \hyperlink{probabilityofdeviation}{Вероятность отклонения относительной частоты} от вероятности события

    $n$ - число испытаний, $p = p(A), \frac{n_A}{n}$ - экспериментальная частота

    $p\left(|\frac{n_A}{n} - p| \leq \varepsilon\right) = p\left(-\varepsilon \leq \frac{n_A}{n} - p \leq \varepsilon\right) \underset{n \to \infty}{\longrightarrow} 2\Phi\left(\frac{\sqrt{n}\varepsilon}{\sqrt{pq}}\right)$

    \hyperlink{lawofbignumbersbernoulli}{Закон больших чисел Бернулли}: $p\left(|\frac{n_A}{n} - p| \leq \varepsilon\right) \underset{n \to \infty}{\longrightarrow} 2 \Phi\left(\frac{\varepsilon}{\sqrt{pq}}\sqrt{n}\right) \to 1$ - 
    закон больших чисел показывает, что вероятность попадания относительной частоты в $\varepsilon$-трубу приближается к 1

    \item \textit{Схемы испытаний: Бернулли, до первого успеха. Биномиальное и геометрическое распределения. Свойство отсутствия последействия.}

    \hyperlink{bernoullischema2}{Схема Бернулли}: $\letsymbol v_n$ - число успехов в серии из $n$ испытаний; 
    $P_n(v_n = k) = C^k_n p^k q^{n - k}, \quad\quad k = 0, 1, \dots, n$

    \hyperlink{binomialdistribution}{Биномиальное распределение}: Соответствие $k \rightarrow C^k_n p^k q^{n - k}, \quad k = 0, \dots, n$ называется биномиальным распределением
    (обозначается $B_{n,p}$ или $B(n, p)$)

    \hyperlink{untilfirstsuccessschema}{Схема до первого успеха}: Пусть проводится бесконечная серия испытаний, которая заканчивается после первого успешного испытания
    под номером $\tau$, тогда вероятность $P(\tau = k) = q^{k - 1} p, \quad\quad k = 1, 2, \dots$

    \hyperlink{geometricdistribution}{Геометрическое распределение}: Соответствие $k \rightarrow q^{k - 1} p, k \in \Natural$ называется геометрическим
    распределение вероятности (обозначается $G_p$ или $G(p)$)

    Геометрическое распределение обладает свойством \enqoute{нестарения} или свойством отсутствия
    последействия: \Ths $\letsymbol P(\tau = k) = q^{k - 1} p, k \in \Natural$. Тогда $\forall n, k \geq 0 \quad P(\tau > n + k \ | \ \tau > n) = P(\tau > k)$

    \item \textit{Урновая схема с возвратом и без возврата. Гипергеометрическое распределение. Теорема об его асимптотическом приближении к биномиальному.}

    \hyperlink{urnschema}{Урновая схема}: В урне $N$ шаров, из которых $K$ шаров белые, $N - K$ - черные.
    Из урны вынимаем (без учета порядка) $n$ шаров. Найти вероятность, что из них $k$ белых

    а) Схема с возвратом (после каждого раза кладем шар обратно). В этом случае вероятность вынуть белый шар одинакова и
    равна $\frac{K}{N}$. Получаем схему Бернулли: $P_n(k) = C^k_n \left(\frac{K}{N}\right)^k \left(1 - \frac{K}{N}\right)^{n - k}$

    б) Схема без возврата - вынутый шар мы выбрасываем, тогда
    $P_{N, K} (n, k) = \frac{C^k_K C^{n - k}_{N - K}}{C^n_N}$

    \hyperlink{hypergeometricdistribution}{Гипергеометрическое распределение}: Соответствие $k \rightarrow \frac{C^k_K C^{n - k}_{N - K}}{C^n_N}, k = 0, \dots, n$ называется гипергеометрическим
    распределением

    \hyperlink{hypergeometricasimptotic}{Теорема о приближении к биномиальному}: \Ths Если $K, N \to \infty$ таким образом, что $\frac{K}{N} \to p \in (0;1)$, а $n$ и $0 \leq k \leq n$ фиксированы, то
    вероятность при гипергеометрическом распределении будет стремиться к биномиальному:
    $P_{N,K} (n, k) = \frac{C^k_K C^{n - k}_{N - K}}{C^n_N} \rightarrow C^k_n \left(\frac{K}{N}\right)^k \left(1 - \frac{K}{N}\right)^{n - k}$

    \item \textit{Схема Пуассона. Формула Пуассона. Оценка погрешности в формуле Пуассона.}
    
    \hyperlink{poissonschema}{Схема Пуассона}: вероятность числа успеха при одном испытании $p_n$ зависит от числа испытаний $n$, причем таким образом, что $n p_n \approx \lambda = const$, 
    $\lambda$ - интенсивность появления редких событий в единицу времени в потоке испытаний. Применимо при $p$ близком к 0 или к 1.

    \hyperlink{poissonformula}{Формула Пуассона}: \Ths Пусть $n \to \infty, p_n \to 0$ таким образом, что $n p_n \to \lambda = const > 0$.
    Тогда вероятность $k$ успехов при $n$ испытаниях: $P_n(k) = C^k_n p_n^k (1 - p_n)^{n - k} \underset{n \to \infty}{\rightarrow} = \frac{\lambda^k}{k!} e^{-\lambda}$

    \hyperlink{errorinpoissonformula}{Оценка погрешности}: \Ths Пусть $v_n$ - число успехов при $n$ испытаниях в схеме Бернулли

    $p$ - вероятность успеха при одном испытании, $\lambda = np$, $A \subset \{0, 1, \dots, n\}$ - произвольное подмножество чисел

    Тогда $|P_n (v_n \in A) - \sum_{k \in A} \frac{\lambda^k}{k!} e^{-\lambda}| \leq \min (p, np^2) = \min (p, p\lambda)$

    \item \textit{Случайные величины, определение. Измеримость функции, ее смысл. Вероятностное пространство $(\Real, B, P)$. Распределение случайной величины.}

    \hyperlink{randomvaluedefinition}{Случайной величиной}, заданной на вероятностном пространстве $(\Omega, \mathcal{F}, p)$, называется
    $\mathcal{F}$-измеримая функция $\xi \ : \Omega \to \Real$, которая сопоставляет каждому элементарному исходу \
    некоторое вещественное число

    \hyperlink{measurabilityoffunction}{Измеримость}: На вероятностном пространстве $(\Omega, \mathcal{F}, p)$ функция $\xi \ : \ \Omega \to \Real$ называется
    $\mathcal{F}$-измеримой, если $\forall x \in \Real \ \{\omega \in \Omega \ | \ \xi(\omega) < x\} \in \mathcal{F}$
    (то есть $\xi^{-1}(y) \in \mathcal{F}$, где $y \in (-\infty; x)$)

    Смысл измеримости: если задана случайная величина $\xi$, то мы можем задать вероятность попадания случайной
    величины в интервал $(-\infty; x)$: $p(\xi \in (-\infty; x)) = p(\{\omega \in \Omega \ | \ \xi(\omega) < x\})$

    \hyperlink{probabilityspacerbp}{Вероятностное пространство $(\Real, B, P)$}: Пусть $\xi$ задана на вероятностном пространстве $(\Omega, \mathcal{F}, p)$, с помощью нее получаем новой вероятностное
    пространство $(\Real, \mathcal{B}(\Real), p_\xi)$, с которым проще работать

    \hyperlink{randomvaluedistribution}{Распределение случайной величины}: Функция $p(B), B \in \mathcal{B}(\Real)$, ставящая в соответствие каждому Борелевскому множеству вероятность,
    называется распределением случайной величины $\xi$

    \item \textit{Дискретные случайные величины. Определение, закон распределения, числовые характеристики.}

    \hyperlink{discreterandomvalue}{Дискретная случайная величина}: Случайная величина $\xi$ имеет дискретное рапределение, если она принимает не более, чем счетное число значений.
    То есть существует конечный или счетный набор чисел $\{x_1, x_2, \dots, x_n, \dots\}$ такой, что $p(\xi = x_i) = p_i > 0$ и $\sum_{i = 0}^\infty p_i = 1$

    Таким образом, дискретная случайная величина (ДСВ) задается законом распределения:

    \begin{tabular}{c|c|c|c|c|cl}
        $\xi$ & $x_1$ & $x_1$ & \dots & $x_n$ & \dots & \text{\qquad   - значения случайной величины} \\
        \cline{1-6}
        $p$   & $p_1$ & $p_1$ & \dots & $p_n$ & \dots & \text{\qquad   - вероятности этих значений}
    \end{tabular}

    \hyperlink{attributesofdiscreterandomvalue}{Характеристики дискретной случайной величины}: 

    Математическим ожиданием $E\xi$ случайной величины $\xi$ называется число
    $E\xi = \sum_{i = 1}^\infty x_i p_i$

    Дисперсией $D\xi$ случайной величины $\xi$ называют среднее квадратов ее отклонения от математического ожидания:
    $D\xi = E (\xi - E\xi)^2$ или $D\xi = \sum_{i = 0}^\infty (x_i - E\xi)^2 p_i$ при условии, что данный ряд сходится

    Дисперсию обычно удобно считать по формуле $D\xi = E\xi^2 - (E\xi)^2 = \sum_{i = 1}^n x^2_i p_i - E\xi^2$

    Средним квадратическим отклонением (СКО) $\sigma_\xi$ называется величина $\sigma_\xi = \sqrt{D\xi}$

    $m_k = E\xi^k$ - момент $k$-ого порядка случайной величины $\xi$ (также называют начальным моментом)

    $\mu_k = E(\xi - E\xi)^k$ - центральный момент $k$-ого порядка

    \item \textit{Свойства математического ожидания и дисперсии дискретной случайной величины.}

    \hyperlink{expectedvalueandvarianceproperties}{Свойства}: 

    \ThNs{1} Случайная величина $\xi$ имеет вырожденное распределение, если $\xi(\omega) = \mathrm{const} \ \ \forall \omega \in \Omega$

    \begin{tabular}{c|c}
        $\xi$ & $C$ \\
        \hline
        $p$   & $1$
    \end{tabular}

    $E\xi = C \qquad D\xi = 0$

    \ThNs{2} Свойство сдвига: $E(\xi + C) = E\xi + C; D (\xi + C) = D\xi$

    \ThNs{3} Свойство растяжения: $E(C\xi) = CE\xi$, $D(C\xi) = C^2 D\xi$

    \ThNs{4} $E(\xi + \eta) = E\xi + E\eta$ (из третьего свойства матожидание - линейная функция)

    \Def Дискретные случайные величины $\xi$ и $\eta$ независимы, если $p(\xi = x_i, \eta = y_i) = p(\xi = x_i) \cdot p(\eta = y_i) \ \forall i, j$.
    То есть случайные величины принимают свои величины независимо друг от друга

    \ThNs{5} Если случайные величины $\xi$ и $\eta$ независимы, то $E(\xi \eta) = E\xi \cdot E\eta$; обратное неверно

    \ThNs{6} $D\xi = E\xi^2 - (E\xi)^2$

    \Def $D(\xi + \eta) = D\xi + D\eta + 2\mathrm{cov} (\xi, \eta)$,
    где $\mathrm{cov}(\xi, \eta) = E(\xi\eta) - E\xi E\eta$ - ковариация случайных величин (равна 0 при независимых величинах) - индикатор наличия связи между случайными величинами

    \ThNs{7} Если случайные величины $\xi$ и $\eta$ независимы, то $D(\xi + \eta) = D\xi + D\eta$

    \ThNs{8} Общая формула дисперсии суммы: $D(\xi_1 + \xi_2 + \dots + \xi_n) = \sum_{i = 1}^n D \xi_i + 2\sum_{i, j (i \neq j)} \mathrm{cov} (\xi_i, \xi_j)$

    \item \textit{Стандартные дискретные распределения и их числовые характеристики (Бернулли, биномиальное, геометрическое, Пуассона).}
    
    \hyperlink{bernoullidistribution}{Распределение Бернулли}: $B_p$ (с параметром $0 < p < 1$), $\xi$ - число успехов при одном испытании, $p$ - вероятность успеха при одном испытании

    \begin{tabular}{c|c|c}
        $\xi$ & $0$        & $1$    \\
        \hline
        $p$   & $1 - P(A)$ & $P(A)$
    \end{tabular} \qquad\qquad $E\xi = p$ \qquad\qquad $D\xi = p(1 - p) = pq$

    \hyperlink{binomialdistributionproperties}{Биномиальное распределение} $B_{n,p}$ (с параметрами $n, p$),
    $\xi$ - число успехов в серии из $n$ испытаний, $p$ - вероятность успеха при одном испытании

    $p(\xi = k) = C^k_n p^k q^{n - k}, \ k = 0, 1, \dots, n \Longleftrightarrow \xi \in B_{n,p}$

    $E\xi_i = p; \quad D\xi_i = pq$

    $E\xi = E\xi_1 + \dots + E\xi_n = p + \dots + p = np$

    $D\xi = D\xi_1 + \dots + D\xi_n = pq + \dots + pq = npq$

    \hyperlink{geometricdistributionproperties}{Геометрическое распределение} $G_p$ (с параметром $p$),
    $\xi$ - номер 1-ого успешного испытания в бесконечной серии

    $p(\xi = k) = q^{k - 1}p, \ k = 1, 2, 3, \dots \Longleftrightarrow \xi \in G_p$

    $E\xi = \frac{1}{p}, D\xi  = \frac{q}{p^2}$

    \hyperlink{poissondistribution}{Распределение Пуассона} $\Pi_\lambda$ (с параметром $\lambda > 0$)

    Случайная величина $\xi$ имеет распределение Пуассона с параметром $\lambda > 0$, если $p(\xi = k) = \frac{\lambda^k}{k!}e^{-\lambda}, \ k = 0, 1, 2, \dots$

    $E\xi = \lambda = np, D\xi = \lambda$


    \item \textit{Функция распределения и ее свойства (в свойствах 4, 5, 6 достаточно привести одно из доказательств).}
    
    \hyperlink{distributionfunction}{Функция распределения} $F_\xi(x)$ случайной величины $\xi$ называется функция $F_\xi(x) = P(\xi < x)$

    \hyperlink{distributionfunctionproperties}{Свойства}: 

    1) $F(x)$ ограничена $0 \leq F(x) \leq 1$

    2) $F(x)$ неубывающая функция: $x_1 < x_2 \Longrightarrow F(x_1) \leq F(x_2)$ 

    3) $p(\alpha \leq \xi < \beta) = F(\beta) - F(\alpha)$

    4) $\lim_{x \to -\infty} F(x) = 0; \quad \lim_{x \to +\infty} F(x) = 1$
    
    5) $F(x)$ непрерывна слева: $F(x_0 - 0) = F(x_0)$

    6) Скачок в точке $x_0$ равен вероятности попадания в данную точку: $F(x_0 + 0) - F(x_0) = p(\xi = x_0)$ или $F(x_0 + 0) = p(\xi = x_0) + p(\xi < x_0) = p(\xi \leq x_0)$

    7) Если функция распределения непрерывна в точке $x = x_0$, то очевидно, что вероятность попадания в эту точка $p(\xi = x_0) = 0$ (следствие из 6 пункта)
    
    8) Если $F(x)$ непрерывна $\forall x \in \Real$, то $p(\alpha \leq \xi < \beta) = p(\alpha < \xi < \beta) = p(\alpha \leq \xi \leq \beta) = p(\alpha < \xi \leq \beta) = F(\beta) - F(\alpha)$
    
    \item \textit{Абсолютно непрерывные случайные величины. Плотность и ее свойства.}

    \hyperlink{continuousdistributionproperties}{Абсолютно непрерывне случайные величины}: Случайная величина $\xi$ имеет абсолютно непрерывное распределение, если существует $f_\xi(x)$ такая, что $\forall B \in \mathcal{B}(\Real)
    \ p(\xi \in B) = \int_B f_\xi(x)dx$

    \hyperlink{densityfunctiondefinition}{Функция плотности}: Функция $f_\xi$ называется плотностью распределения случайной величины

    \hyperlink{densityfunctionproperties}{Свойства}:

    1) Вероятносто-геометрический смысл плотности: $p(\alpha \leq \xi < \beta) = \int_{\alpha}^\beta f_\xi(x) dx$

    2) Условие нормировки: $\int_{-\infty}^{+\infty} f_\xi(x)dx = 1$

    3) $F_\xi(x) = \int_B f_\xi(x)dx$

    4) $F_\xi(x)$ непрерывна 

    5) $F_\xi(x)$ дифференцируема почти везде и $f_\xi(x) = F^\prime_\xi(x)$ для почти всех $x$

    6) $f_\xi(x) \geq 0$ по определению и как производная неубывающей $F_\xi(x)$

    7) $p(\xi = x) = 0 \ \forall x \in \Real$ - так как $F_\xi(x)$ непрерывна

    8) $p(\alpha \leq \xi < \beta) = p(\alpha < \xi < \beta) = p(\alpha \leq \xi \leq \beta) = p(\alpha < \xi \leq \beta) = F(\beta) - F(\alpha)$

    9) \Ths Если $f(x) \geq 0$ и $\int_{-\infty}^{\infty} f(x)dx = 1$ (выполнены свойства 2 и 6), то $f(x)$ - плотность некоторого распределения

    \item \textit{Числовые характеристики абсолютно непрерывной случайной величины, их свойства.}

    \hyperlink{attributesofcontinuousrandomvariable}{Характеристики}:

    Математическим ожиданием $E\xi$ случайной абсолютно непрерывной величины $\xi$ называется величина $E\xi = \int_{-\infty}^{\infty} xf_\xi(x) dx$ 
    при условии, что данный интеграл сходится абсолютно, то есть $\int_{-\infty}^\infty |x|f_\xi(x)dx < \infty$

    Дисперсией $D\xi$ случайной величины $\xi$ называется величина $D\xi = E(\xi - E\xi)^2 = \int_{-\infty}^\infty (x - E\xi)^2 f_\xi(x) dx$ при условии,
    что данный интеграл сходится. Вычислять удобно по формуле $D\xi = E\xi^2 - (E\xi)^2 = \int_{-\infty}^\infty x^2 f_\xi(x)dx - (E\xi)^2$

    Среднее квадратическое отклонение $\sigma_\xi = \sqrt{D\xi}$ определяется, как корень дисперсии

    $m_k = E\xi^k = \int_{-\infty}^\infty x^k f_\xi(x)dx$ - момент $k$-ого порядка

    $\mu_k = E(\xi - E\xi)^k = \int_{-\infty}^\infty (x - E\xi)^k f_\xi(x)dx$ - центральный момент $k$-ого порядка
    
    Медианой $Me$ абсолютно непрерывной случайной величины $\xi$ называется значение случайной величины $\xi$, такое что $p(\xi < Me) = p(\xi > Me) = \frac{1}{2}$
    
    Модой $Mo$ случайной величины $\xi$ называется точка локального максимума плотности

    \item \textit{Равномерное распределение. }

    \hyperlink{uniformdistribution}{Равномерное распределение}: Случайная величина $\xi$ имеет равномерное распределение $\xi \in U(a, b)$, если ее плотность
    на этом отрезке постоянна. Получаем функцию плотности $f_\xi(x) = \begin{cases}0, \quad x < a \\ \frac{1}{b - a}, \quad a \leq x < b \\ 0 \quad x \geq b\end{cases}$ \hfill {\scriptsize $\frac{1}{b - a}$ из усл. нормировки}

    $F(x) = \int_{-\infty}^\infty f(x)dx = \begin{cases}0, \quad x < a \\ \frac{x - a}{b - a}, \quad a \leq x < b \\ 1 \quad x \geq b\end{cases}$

    $E\xi = \frac{a + b}{2}, \quad D\xi = \frac{(b - a)^2}{12}, \quad \sigma = \frac{b - a}{2\sqrt{3}}$

    $p(\alpha < \xi < \beta) = \frac{\beta - \alpha}{b - a}$ при условии, что $\alpha, \beta \in [a, b]$

    \item \textit{Показательное распределение. Свойство нестарения.}

    \hyperlink{exponentialdistribution}{Показательное распределение}: Случайная величина $\xi$ имеет показательное (или экспоненциальное) распределение с параметром $\alpha > 0$ (обозн. $\xi \in E_\alpha$),
    если ее плотность имеет вид: $f_\xi(x) = \begin{cases}0, \quad x < 0 \\ \alpha e^{-\alpha x}, \quad x \geq 0\end{cases}$

    $F_\xi(x) = \begin{cases}0, \quad x < 0 \\ \int_0^x \alpha e^{-\alpha x} = 1 - e^{-\alpha x}, \quad x \geq 0\end{cases}$

    $E\xi = \frac{1}{\alpha}, \quad D\xi = \frac{1}{\alpha^2}, \quad \sigma = \frac{1}{\alpha}$

    $p(\alpha < \xi < \beta) = F(b) - F(a) = e^{-a\alpha} - e^{-b\alpha} \quad\quad\quad a, b \geq 0$

    Из непрерывных случайных величин только показательная обладает свойством нестарения:
    \Ths $\letsymbol \xi \in E_\alpha$. Тогда $p(\xi > x + y \ | \ \xi > x) = p(\xi > y) \quad\quad \forall x, y > 0$

    \item \textit{Нормальное распределение. Стандартное нормальное распределение, его числовые характеристики.}

    \hyperlink{normaldistribution}{Нормальное распределение}: Случайная величина $\xi$ имеет нормальное распределение с параметрами $a$ и $\sigma^2$ (обозн. $\xi \in N(a, \sigma^2)$), если
    ее плотность имеет вид: $f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - a)^2}{2\sigma^2}}$

    $F(x) = \frac{1}{\sigma \sqrt{2\pi}} \int_{-\infty}^x e^{-\frac{(t - a)^2}{2\sigma^2}} dt$

    $E\xi = a, \quad D\xi = \sigma^2, \quad \sigma = \sigma$

    \hyperlink{standardnormaldistribution}{Стандартным нормальным распределением} называется нормальное распределение с параметрами $a = 0, \sigma^2 = 1$: $\xi \in N(0, 1)$

    Плотность: $\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ - функция Гаусса

    Распределение: $F_0(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-\frac{z^2}{2}} dz$ - функция стандартного нормального распределения

    $E\xi = 0; \ D\xi = 1$

    \item \textit{Связь между стандартным нормальным и нормальным распределениями. Следствия.}

    \hyperlink{connectionbetweennormalandstandard}{Связь}: 

    1) $\letsymbol \xi \in N(a, \sigma^2)$. Тогда $F_\xi(x) = F_0\left(\frac{x - a}{\sigma}\right)$

    2) Если $\xi \in N(a, \sigma^2)$, то $\eta = \frac{\xi - a}{\sigma} \in N(0, 1)$ (процесс $\xi \to \eta$ называется стандартизацией)

    3) $\letsymbol \xi \in N(a, \sigma^2)$. Тогда $p(\alpha < \xi < \beta) = \Phi\left(\frac{\beta - a}{\sigma}\right) - \Phi\left(\frac{\alpha - a}{\sigma}\right)$

    4) Вероятность попадания в симметричный интервал (вероятность отклонения случайной величины от матожидания) 
    $p(|\xi - a| < t) = 2\Phi\left(\frac{t}{\sigma}\right)$

    5) Правило 3 \enquote{сигм}: $p(|\xi - a| < 3\sigma) \approx 0.9973$ - попадание случайной величины нормального распределения в интервал $(a - 3\sigma, a + 3\sigma)$ близко к 1

    6) Свойство линейности: если случайная величина $\xi \in N(a, \sigma^2)$, то $\eta = \gamma \xi + b \in N(a \gamma + b, \gamma^2 \sigma^2)$ 
    
    7) Устойчивость относительно суммирования: если случайные величины $\xi_1 \in N(a_1, \sigma_1^2), \xi_2 \in N(a_2, \sigma_2^2)$, и они независимы, то $\xi_1 + \xi_2 \in N(a_1 + a_2, \sigma^2_1 + \sigma^2_2)$

    
    \item \textit{Сингулярные распределения. Теорема Лебега (без док-ва).}

    \hyperlink{singulardistribution}{Сингулярное распределение}: Случайная величина $\xi$ имеет сингулярное распределение, если $\exists B$ - Борелевское множество с нулевой мерой Лебега $\lambda(B) = 0$, такое что $p(\xi \in B) \in 1$, но $P(\xi = x) = 0 \ \, \forall x \in B$

    \hyperlink{lebesguetheorem}{Теорема Лебега}: \ThNs{Лебега}

    $\letsymbol F_\xi(x)$ - функция распределения $\xi$. Тогда $F_\xi(x) = p_1 F_1(x) + p_2 F_2(x) + p_3 F_3(x)$, где $p_1 + p_2 + p_3 = 1$

    $F_1$ - функция дискретного распределения

    $F_2$ - функция абсолютно непрерывного распределения

    $F_3$ - функция сингулярного распределения

    То есть существуют только дискретное, абсолютно непрерывное, сингулярное распределения и их смеси

    \item \textit{Преобразования случайных величин. Стандартизация случайной величины. }

    \hyperlink{standardizationofrandomvalue}{Стандартизация}: Пусть имеется случайная величина $\xi$. Соответствующей ей стандартной величиной называется
    случайная величина $\eta = \frac{\xi - E\xi}{\sigma}$

    $E\eta = 0; D\eta = 1$

    \hyperlink{randomvaluetransformation}{Преобразование}: Если $\xi$ - дискретная случайная величина, то ее законы распределения находятся просто: значения $x_i$ в верхней строке заменяем $g(x_i)$, вероятности остаются прежние.
    
    \item \textit{Теорема о монотонном преобразовании. Линейное преобразование случайной величины. (без док-ва).}
    
    \hyperlink{monotonoustransformationtheorem}{Теорема о монотонном преобразовании}: \Ths Пусть $f_\xi(x)$ - плотность случайной величины $\xi$, $g(x)$ - строго монотонная функция. Тогда 
    случайная величина $\eta = g(\xi)$ имеет плотность $f_\eta(x) = |h^\prime(x)| f_\xi(h(x)), \qquad\qquad \text{где } h(g(x)) = x$

    Если $g(x)$ не является монотонной функцией, то поступаем следующим образом: разбиваем $g(x)$ на интервалы монотонности, 
    для каждого $i$-ого интервала находим $h_i(x)$ и плотность случайной величины ищем по \textit{формуле Смирнова}: 
    $f_\eta(x) = \sum_{i = 0}^n |h_i^\prime(x)| f_\xi(h_i(x))$
    
    \hyperlink{lineartransformation}{Линейное преобразование}: \Ths Пусть $\xi$ имеет плотность $f_\xi(x)$, тогда $\eta = a\xi + b$, где $a \neq 0$, имеет плотность $f_\eta(x) = \frac{1}{|a|}f_\xi\left(\frac{x - b}{a}\right)$
    
    \item \textit{Квантильное преобразование. Моделирование случайной величины с помощью датчика случайных чисел.}

    \hyperlink{quantiletransformation}{Квантильное преобразование}: Пусть функция распределения случайной величины $\xi$ $F_\xi(x)$ - непрерывная функция. 
    Тогда $\eta = F(\xi) \in U(0, 1)$ - стандартное равномерное распределение

    Пусть $\eta \in U(0, 1)$ - стандартное равномерное распределение, $F(x)$ - произвольная функция распределения. 
    Тогда $\xi = F^{-1}(\eta)$ имеет функцию распределения $F(x)$

    Преобразование $\xi = F^{-1}(\eta)$ называют квантильным

    Смысл: датчики случайных чисел имеют стандартное равномерное распределение, из теоремы следует, что при помощи
    датчика случайных чисел и квантильного преобразования мы сможем смоделировать любое нужно распределение


    \item \textit{Виды сходимостей случайных величин, связь между ними. Теорема об эквивалентности сходимостей к константе (все без док-ва).}

    \hyperlink{convergencetypes}{Виды сходимостей}:

    \begin{itemize}
        \item Сходимость \enquote{почти наверное}

        \Defs Последовательность случайных величин $\{\xi_n\}$ сходится \enquote{почти наверное} к случайной величине $\xi$ при $n \to \infty$ ($\xi_n \overset{\text{п. н.}}{\longrightarrow} \xi$), 
        если $p(\omega \in \Omega \ | \ \xi_n(\omega) \underset{n \to \infty}{\longrightarrow} \xi(\omega)) = 1$

        \item Сходимость по вероятности

        \Defs Последовательность случайных величин $\{\xi_n\}$ сходится по вероятности к случайной величине $\xi$ при $n \to \infty$
        ($\xi_n \overset{p}{\longrightarrow} \xi$), если $\forall \varepsilon > 0 \quad p(|\xi_n - \xi| < \varepsilon) \underset{n \to \infty}{\longrightarrow} 1$
        
        \item Слабая сходимость

        \Defs Последовательность случайных величин $\xi_n$ слабо сходится к случайной величине $\xi$ при $n \to \infty$
        ($\xi_n \rightrightarrows \xi$), если $F_{\xi_n}(x) \longrightarrow F_\xi(x) \forall x$, где $F_\xi(x)$ - непрерывна
    \end{itemize}

    \hyperlink{connectionbetweenconvergencetypes}{Связь}: 

    \Ths $\xi_n \overset{\text{п. н.}}{\longrightarrow} \xi \Longrightarrow \xi_n \overset{p}{\longrightarrow} \xi \Longrightarrow \xi_n \rightrightarrows \xi$

    \Ths Если $\xi_n \rightrightarrows C = \mathrm{const}$, то $\xi_n \overset{p}{\longrightarrow} C$

    \Nota В общем случае не только из слабой сходимости не следует сходимость по вероятности, но и бессмысленно говорить
    об этом, так как слабая сходимость - это сходимость не случайных величин, а их распределений

    \item \textit{Математическое ожидание преобразованной случайной величины. Свойства моментов.}
    
    \hyperlink{expectedvalueoftransformedvariable}{Матожидание}: \Ths Если $\xi$ - дискретная случайная величина, то $Eg(\xi) = \sum_{i = 1}^\infty g(x_i) \cdot p(\xi = x_i)$

    Для непрерывной случайной величины $Eg(\xi) = \int_{-\infty}^{\infty} g(x) f_\xi(x) dx$

    \hyperlink{momentsproperties}{Свойства моментов}: 1) Если $\xi \geq 0$, то $E\xi \geq 0$

    2) Если $\xi \leq \eta$, то $E\xi \leq E\eta$

    3) Если $|\xi| \leq |\eta|$, то $E|\xi|^k \leq E|\eta|^k$

    4) Если существует момент $m_t$ случайной величины $\xi$, то существует $m_s$ при $s < t$ (при условии, что интеграл/сумма сходятся)

    
    \item \textit{Неравенство Йенсена, следствие.}

    \hyperlink{jensensinequality}{Неравенство Йенсена}: \Ths Пусть функция $g(x)$ выпукла вниз, тогда для любой случайной величины $\xi$ $Eg(\xi) \geq g(E\xi)$

    \Nota Если $g(x)$ выпукла вверх, знак неравенства меняется

    Следствие: $Ee^\xi \geq e^{E\xi}, \quad E\xi^2 \geq (E\xi)^2, \quad E|\xi| \geq |E\xi|, \quad E\ln(\xi) \leq \ln(E\xi), \quad E\frac{1}{\xi} \geq \frac{1}{E\xi}$ при $\xi > 0$

    \item \textit{Неравенства Маркова, Чебышева, правило трех сигм.}

    Для $\xi$, у которой существует матожидание, верно: 

    \hyperlink{markovsinequality}{Неравенство Маркова}: \Ths $p(|\xi| \geq \varepsilon) \leq \frac{E|\xi|}{\varepsilon} \quad \forall \varepsilon > 0$
    
    \hyperlink{chebyshevsinequality}{Неравенство Чебышева}: \Ths $P(|\xi - E\xi| \geq \varepsilon) \leq \frac{D\xi}{\varepsilon^2}$
    
    \hyperlink{ruleofthreesigmas}{Правило \enquote{трех сигм}}: \Ths $P(|\xi - E\xi| \geq 3\sigma) \leq \frac{1}{9}$

    \item \textit{Среднее арифметическое одинаковых независимых случайных величин. Закон больших чисел Чебышева.}

    \hyperlink{averagevalueofrandomvariables}{Среднее арифметическое}: $\frac{S_n}{n} = \frac{\xi_1 + \dots + \xi_n}{n}$

    $E\left(\frac{S_n}{n}\right) = \frac{1}{n} (E\xi_1 + \dots + E\xi_n) = \frac{1}{n} na = a = E\xi_1$ - математическое ожидание не меняется

    $D\left(\frac{S_n}{n}\right) = \frac{1}{n^2} (D\xi_1 + \dots + D\xi_n) = \frac{1}{n^2} nd = \frac{d}{n} = \frac{D\xi_1}{n}$ - дисперсия уменьшилась в $n$ раз

    $\sigma\left(\frac{S_n}{n}\right) = \frac{\sigma}{\sqrt{n}}$ - СКО уменьшилось в $\sqrt{n}$ раз

    \hyperlink{lawofbignumberschebyshev}{Закон больших чисел Чебышев}: \Ths Пусть $\xi_1, \dots, \xi_n, \dots$ - последовательность независимых одинаково распределенных с конечным вторым моментом,
    тогда $\frac{\xi_1 + \dots + \xi_n}{n} \overset{p}{\underset{n \to \infty}{\longrightarrow}} E\xi_1$

    \item \textit{Вывод закона больших чисел Бернулли из закона больших чисел Чебышева. Законы больших чисел Хинчина и Колмогорова (только формулировки).}

    \hyperlink{lawofbignumbersbernoulli2}{ЗБЧ Бернулли}: \Ths Пусть $v_n$ - число успехов из $n$ независимых испытаний, $p = P(A)$ - вероятность успеха при одном испытании.
    Тогда $\frac{v_n}{n} \overset{p}{\longrightarrow} P(A)$

    \begin{MyProof}
        $v_n = \xi_1 + \dots + \xi_n$, где $\xi_i \in B_p$ - число успехов при $i$-ом испытании

        $E\xi_i = p; D\xi_i = pq$

        $\frac{v_n}{n} \overset{p}{\longrightarrow} E\xi_1 = p$

        $p\left(\left|\frac{v_n}{n} - p\right| \geq \varepsilon\right) \leq \frac{D\xi_1}{n\varepsilon^2} = \frac{pq}{n\varepsilon^2}$
    \end{MyProof}

    \hyperlink{lawofbignumberskhinchin}{ЗБЧ Хинчина}: \Ths $v_n = \xi_1 + \dots + \xi_n$ последовательность независимых одинаково распределенных случайных величин с конечным первым моментом, тогда
    $\frac{\xi_1 + \dots + \xi_n}{n} \overset{p}{\longrightarrow} E\xi_i$

    \hyperlink{lawofbignumberskolmogorov}{ЗБЧ Колмогорова}: В условиях теоремы Хинчина $\frac{\xi_1 + \dots + \xi_n}{n} \overset{\text{п.н.}}{\longrightarrow} E\xi_1$

    \item \textit{Совместные распределения случайных величин. Функция совместного распределения, ее свойства. Независимость случайных величин.}
    
    \hyperlink{jointdistribution}{Совместное распределение}: Случайным вектором $\vec{\xi} = (\xi_1, \xi_2, \dots, \xi_n)$ называется упорядоченный набор случайных величин, заданных
    на одном вероятностном пространстве

    Случайный вектор задает отображение $(\xi_1, \dots, \xi_n) (\omega) : \Omega \longrightarrow \Real^n$

    \hyperlink{jointdistributionfunction}{Функция совместного распределения}: Функцией совместного распределения случайных величин $\xi_1, \xi_2, \dots, \xi_n$ называется функция 
    $F_{\xi_1, \xi_2, \dots, \xi_n}(x_1, x_2, \dots, x_n) = P(\xi_1 < x_1, \xi_2 < x_2, \dots, \xi_n < x_n)$

    \hyperlink{jointdistributionfunctionproperties}{Свойства}: 

    \begin{enumerate}
        \item $0 \leq F_{\xi, \eta}(x, y) \leq 1$
        \item $F_{\xi, \eta}(x, y)$ - неубывающая по каждому аргументу
        \item $\lim_{x \to -\infty} F_{\xi, \eta}(x, y) = \lim_{y \to -\infty} F_{\xi, \eta}(x, y) = 0, $
        $\lim_{\substack{x \to \infty \\ y \to \infty}} F_{\xi, \eta}(x, y) = 1$

        \item Восстановление маргинального (частного) распределения: 
        $\lim_{x \to \infty} F_{\xi, \eta}(x, y) = F_\eta(y)$, и наоборот - $\lim_{y \to \infty} F_{\xi, \eta}(x, y) = F_\xi(x)$

        \item $F_{\xi, \eta}(x, y)$ - непрерывна слева по каждому аргументу

        \item $P(x_1 \leq \xi < x_2, y_1 \leq \eta < y_2) = F_{\xi, \eta}(x_2, y_2) - F_{\xi, \eta}(x_2, y_1) - F_{\xi, \eta}(x_1, y_2) + F_{\xi, \eta}(x_1, y_1)$
    \end{enumerate}

    \hyperlink{randomvariablesindependence}{Независимость величин}: Случайные величины $\xi_1, \dots, \xi_n$ независимы в совокупности, если для любого набора Борелевских множеств из
    $\mathcal{B}(\Real^n)$, $B_1, B_2, \dots, B_n$ верно $p(\xi_1 \in B_1, \xi_2 \in B_2, \dots, \xi_n \in B_n) = p(\xi_1 \in B_1) \cdot p(\xi_2 \in B_2) \cdot \dots \cdot p(\xi_n \in B_n)$

    Случайные величины $\xi_1, \xi_2, \dots, \xi_n$ попарно независимы, если независимы любые две из них

    \item \textit{Дискретная система двух случайных величин. Закон совместного распределения. Маргинальные распределения.}

    \hyperlink{discretesystemoftwovariables}{Дискретная система}: Случайные величины $\xi, \eta$ имеют совместное дискретное распределение, если случайный вектор $(\xi, \eta)$
    принимает не более, чем счетное число значений, то есть существует конечный или счетный набор пар чисел $(x_i, y_i)$, 
    таких что $P(\xi = x_i, \eta = y_i) > 0, \sum_{i, j} P(\xi = x_i, \eta = y_i) = 1$
    
    Таким образом двумерная дискретная случайная величина задается законом распределения - таблице вероятностей

    \begin{tabular}{c|c|c|c|c}
        $\xi \backslash \eta$ & $y_1$ & $y_2$ & $\dots$ & $y_m$ \\
        \hline
        $x_1$ & $p_{11}$ & $p_{12}$ & $\dots$ & $p_{1m}$ \\
        \hline
        $x_2$ & $p_{21}$ & $p_{22}$ & $\dots$ & $p_{2m}$ \\
        \hline
        $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
        \hline
        $x_n$ & $p_{n1}$ & $p_{n2}$ & $\dots$ & $p_{nm}$ \\
    \end{tabular}

    Зная общий закон распределения, можно восстановить частное (маргинальное) распределение по формулам: 

    $p_i = \sum_{j = 1}^m p_{i, j} \qquad q_j = \sum_{i = 1}^n p_{i, j}$

    \item \textit{Абсолютно непрерывная система двух случайных величин. Плотность совместного распределения, ее свойства.}
    
    \hyperlink{continuoussystemoftwovariables}{Непрерывная система}: Случайные величины $\xi$ и $\eta$ имеют абсолютно непрерывное совместное распределение, если
    $\exists f_{\xi, \eta}(x, y)$, такая что $\forall B \in \mathcal{B}(\Real^2) \ P((\xi, \eta) \in B) = \iint_B f_{\xi, \eta}(x, y) dxdy$

    Функцию $f_{\xi, \eta}(x, y)$ будем называть функцией плотности совместного распределения случайных величин $\xi$ и $\eta$

    \hyperlink{densityfunctionpropertiesincontinuoussystem}{Свойства}: 

    
    \begin{enumerate}
        \item $f_{\xi, \eta}(x, y) \leq 0$
        \item Условие нормировки: $\iint_{\Real^2} f_{\xi, \eta}(x, y) dxdy = 1$
        \item $F_{\xi, \eta} = \int_{-\infty}^x \int_{-\infty}^y f_{\xi, \eta}(x, y) dydx$

        \item $f_{\xi, \eta}(x, y) = \frac{\partial^2 F_{\xi, \eta}(x, y)}{\partial x \partial y}$
        
        \item Если случайные величины $\xi, \eta$ имеют абсолютно непрерывное совместное распределение с плотностью $f(x, y)$, 
        то маргинальное распределение величин $\xi, \eta$ также имеют абсолютно непрерывное распределение
        с плотностями $f_\xi(x) = \int_{-\infty}^\infty f_{\xi, \eta}(x, y) dy, f_\eta(y) = \int_{-\infty}^\infty f_{\xi, \eta}(x, y) dx$

        \item Так как вероятность попадания в Борелевские множества полностью задается функцией распределения, 
        то условие независимости случайных величин эквивалентно следующему:

        $\xi_1, \xi_2, \dots, \xi_n$ независимы, если функция общего распределения распадается в произведение 
        отдельных функцию распределения
    
        $F_{\xi_1, \xi_2, \dots, \xi_n}(x_1, x_2, \dots, x_n) = F_{\xi_1}(x_1) \cdot F_{\xi_2}(x_2) \cdot \dots \cdot F_{\xi_n}(x_n)$

        \item \textit{Равносильное определение}: абсолютно непрерывные случайные величины $\xi_1, \dots, \xi_n$ независимы в совокупности тогда и только тогда, 
        когда плотность совместного распределения $f_{\xi_1, \xi_2, \dots, \xi_n}(x_1, x_2, \dots, x_n) = f_{\xi_1}(x_1) \cdot f_{\xi_2}(x_2) \cdot \dots \cdot f_{\xi_n}(x_n)$
    \end{enumerate}

    \item \textit{Функции от двух случайных величин. Теорема о функции распределения. Формула свертки.}
    
    \hyperlink{functionoftworandomvariables}{Функция от двух случайных величин}: \Ths Пусть $\xi_1, \xi_2$ - случайные величины с общем плотностью $f_{\xi_1, \xi_2}(x, y)$, и есть функция
    $g(x, y) \ : \ \Real^2 \rightarrow \Real$. Тогда случайная величина $\eta = g(\xi_1, \xi_2)$ имеет
    функцию распределения $F_{\eta}(z) = \iint_{D_z} f(x, y)dxdy$, 
    где $D_z = \{(x, y) \in \Real^2 \ | \ g(x, y) < z\}$

    \hyperlink{convolutionformula}{Плотность суммы}: \Ths $\letsymbol \xi_1, \xi_2$ - независимые абсолютно непрерывные случайные величины с плотностями
    $f_{\xi_1}(x)$ и $f_{\xi_2}(y)$

    Тогда плотность суммы $\xi_1 + \xi_2$ равна $f_{\xi_1 + \xi_2}(t) = \int_{-\infty}^\infty 
    \underset{\text{т. н. свертка}}{\underbrace{f_{\xi_1}(x) f_{\xi_2}(t - x)}} dx$

    \item \textit{Суммы стандартных распределений, устойчивость по суммированию (биномиальное, Пуассона, стандартное нормальное).}

    \hyperlink{sumofstandarddistributions}{Суммы стандартных распределений}: 
    \ExNs{1} $\xi \in B_{n, p}; \eta \in B_{m, p}$. Тогда ясно, что $\xi + \eta \in B_{n + m, p}$ 
    (по определению биномиального распределения $B_{n, p}$ - число успехов из $n$ испытаний, где $p$ - вероятность успеха)

    \ExNs{2} $\xi \in \Pi_{\lambda}, \eta \in \Pi_{\mu}$, они независимы. Тогда $\xi + \eta \in \Pi_{\lambda + \mu}$

    \ExNs{3} $\xi, \eta \in N(0, 1)$ и независимы. Тогда $\xi + \eta \in N(0, 2)$

    \ExNs{4} В общности для независимых $\xi \in N(a_1, \sigma^2_1), \eta \in N(a_2, \sigma_2^2) \ \xi + \eta \in N(a_1 + a_2, \sigma_1^2 + \sigma_2^2)$ 

    \ExNs{5} Равномерное распределение неустойчиво относительно суммирования, контрпример:

    $\xi, \eta \in U(0, 1)$ - независимы

    $\forall x, y \in [0, 1] \ f_{\xi}(x) = 1, f_\eta(y) = 1$ и $f_{\xi, \eta}(x, y) = 1$

    По первой теореме $F_{\xi, \eta}(x, y) = \iint_{D_z} f_{\xi, \eta}(x, y) dxdy = \iint_{D_z} dxdy = S_{D_z}$, где $D_z = \{(x, y) \ | \ x + y < z\}$

    \item \textit{Условные распределения и условные математические ожидания. Случаи дискретной и абсолютно непрерывной систем двух случайных величин.}

    \hyperlink{conditionaldistribution}{Условным распределением} случайной величины из системы случайных величин $(\xi, \eta)$ 
    называется ее распределение, найденное при условии, что другая случайная величина приняла 
    определенное значение. Обозначается $\xi | \eta = y$

    Условным математическим ожиданием (обозначается $E(\xi | \eta = y)$) называется 
    математическим ожиданием случайной величины $\xi$ при соответствующем условном распределении
 
    \hyperlink{conditionaldistributionindiscretesystem}{Дискретная система}: 
    Пусть $(\xi, \eta)$ задана законом распределения:

    \begin{tabular}{c|c|c|c|c}
        $\xi \backslash \eta$ & $y_1$ & $y_2$ & $\dots$ & $y_m$ \\
        \hline
        $x_1$ & $p_{11}$ & $p_{12}$ & $\dots$ & $p_{1m}$ \\
        \hline
        $x_2$ & $p_{21}$ & $p_{22}$ & $\dots$ & $p_{2m}$ \\
        \hline
        $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
        \hline
        $x_n$ & $p_{n1}$ & $p_{n2}$ & $\dots$ & $p_{nm}$ \\
    \end{tabular}

    Вероятности условных распределений считаем по формулам:

    $\xi | \eta = y_j$: $p_i = p(\xi = x_i \ | \ \eta = y_j) = \frac{p(\xi = x_i, \eta = y_j)}{p(\eta = y_j)} = \frac{p_{ij}}{q_j} = \frac{p_{ij}}{\sum_i p_{ij}}$

    $\eta | \xi = x_i$: $q_j = p(\eta = y_j \ | \ \xi = x_i) = \frac{p(\xi = x_i, \eta = y_j)}{p(\xi = x_i)} = \frac{q_{ij}}{p_i} = \frac{q_{ij}}{\sum_j p_{ij}}$

    Матожидание $E(\xi | \eta = y_j) = \sum_i x_i p(\xi = x_i, \eta = y_j)$

    \hyperlink{conditionaldistributionincontinuoussystem}{Непрерывная система}: 
    Пусть $(\xi, \eta)$ задана плотностью $f_{\xi, \eta}(x, y)$ совместного распределения, тогда плотность 
    условного распределения $\xi | \eta = y$: $f(x | y) = \frac{f_{\xi, \eta}(x, y)}{\int_\Real f_{\xi, \eta}(x, y)dx} = \frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)}$

    \Defs Функция $f(x | y) = \frac{f_{\xi, \eta}(x, y)}{f_{\eta}(y)}$ называется условной плотностью

    \Defs Условное математические ожидание вычисляется по формуле $E(\xi | \eta = y) = \int_{-\infty}^\infty xf(x | y)dx$

    \item \textit{Пространство случайных величин. Скалярное произведение, неравенство Коши-Буняковского-Шварца. }

    \hyperlink{spaceofrandomvariables}{Пространство} $L_2 (\Omega, \mathcal{F}, P) = \{\xi \ | \ D\xi < \infty\}$ - множество случайных величин 
    на данном пространстве с конечной дисперсией

    \hyperlink{scalarproductoftwovariables}{Скалярным произведением} случайных величин $\xi$ и $\eta$ из $L_2(\Omega, \mathcal{F}, P)$ 
    называется число $(\xi, \eta) = E(\xi\eta)$

    \hyperlink{cauchybunyakovskyschwarzinequality}{Неравенство Коши-Буняковского-Шварца}: \Ths Пусть случайные величины $\xi$ и $\eta$ имеют конечный второй момент, тогда 
    $|E(\xi, \eta)| \leq \sqrt{E\xi^2 \cdot E\eta^2}$ (или $|(\xi, \eta)| \leq \|\xi\|\cdot\|\eta\|$)

    Причем $|E(\xi, \eta)| = \sqrt{E\xi^2 \cdot E\eta^2} \Longleftrightarrow \eta = C\xi$, где $C = \mathrm{const}$

    \item \textit{Условное математическое ожидание как случайная величина, его свойства. Формула полного математического ожидания.}

    \hyperlink{conditionalexpectedvalue}{Условным математическим ожиданием} (УМО, обозначается $E(\xi|\eta) = \hat{\xi}$) случайной величины $\xi$
    относительно случайной величины $\eta$ называется ортогональная проекция случайной величины $\xi$ на $L(\eta)$ 
    
    \hyperlink{conditionalexpectedvalueproperties}{Свойства}:

    \begin{enumerate}
        \item Тождество ортопроекций: $\letsymbol \hat{\xi} \in L(\eta)$, тогда $\hat{\xi} = E(\xi|\eta) \Longleftrightarrow E(\xi\cdot g(\eta)) = E(\hat{\xi}\cdot g(\eta)) \ \forall g(\eta) \in L(\eta)$

        \item Формула полного математического ожидания

        $E\xi = E(E(\xi|\eta))$ или $E\xi = E\hat{\xi}$

        \Nota При распределении Бернулли получаем обычную формулу полной вероятности

        \item Линейность: $E(C_1\xi_1 + C_2\xi_2 \ | \ \eta) = C_1 E(\xi_1|\eta) + C_2 E(\xi_2|\eta)$

        \item Если $\xi$ и $\eta$ независимы, то $E(\xi|\eta) = E\xi$

        \item Если $\xi$ и $\eta$ независимы, то $(\xi - E\xi) \perp g(\eta) \ \forall g(\eta) \in L(\eta)$, 
        в частности $(\xi - E\xi) \perp \eta$
    \end{enumerate}

    \item \textit{Условная дисперсия. Закон полной дисперсии. Смысл второго слагаемого в разложении дисперсии.}

    \hyperlink{conditionalvariance}{Условной дисперсией} случайной величины $\xi$ относительно случайной величины $\eta$ называется случайная величина 
    $D(\xi | \eta) = E((\xi - E(\xi | \eta))^2 | \eta)$
    
    \hyperlink{lawoffullvariance}{Закон полной дисперсии}: \Ths $D\xi = E(D(\xi | \eta)) + D(E(\xi | \eta))$

    \underline{Следствие и смысл}: 
    
    \begin{itemize}
        \item Если $\xi$ и $\eta$ независимы (некоррелированы), то $D(E(\xi | \eta)) = D(E\xi) = 0$ и $D\xi = E(D(\xi | \eta))$

        \item Если имеется функциональная зависимость (то есть $\xi = g(\eta)$), то $D(E(\xi | \eta)) = D(E(g(\eta) | \eta)) = 
        D(g(\eta)) = D\xi$
    \end{itemize}

    \item \textit{Числовые характеристики зависимости случайных величин. Ковариация, ее свойства. Коэффициент корреляции, его свойства. Корреляция случайных величин.}

    \hyperlink{covariance}{Ковариацией} $\mathrm{\cov}(\xi, \eta)$ называется величина $\mathrm{cov}(\xi, \eta) = E((\xi - E\xi)(\eta - E\eta))$

    Свойства:

    \begin{enumerate}
        \item $\mathrm{cov} (\xi, \eta) = E(\xi\eta) - E\xi E\eta$

        \item $\mathrm{cov} (\xi, \xi) = D\xi$

        \item $\mathrm{cov}(\xi, \eta) = \mathrm{cov}(\eta, \xi)$

        \item $\mathrm{cov}(C_1 \xi_1 + C_2 \xi_2, \eta) = C_1 \mathrm{cov}(\xi_1, \eta) + C_2 \mathrm{cov}(\xi_2, \eta)$

        \item $D(\xi + \eta) = D\xi + D\eta + 2\mathrm{cov}(\xi, \eta)$

        \item $D(\xi_1 + \dots + \xi_n) = \sum_{i = 1}^n D\xi_i + 2\sum_{i < j} \mathrm{cov}(\xi_i, \xi_j) = \sum_{i, j = 1}^{n} \mathrm{cov}(\xi_i, \xi_j)$

        \item \begin{enumerate}
            \item Если $\xi$ и $\eta$ - независимы, то $\mathrm{cov}(\xi, \eta) = 0$

            \item Если $\mathrm{cov}(\xi, \eta) \neq 0$, то $\xi$ и $\eta$ - зависимы

            \item Если $\mathrm{cov}(\xi, \eta) = 0$, то неясно
        \end{enumerate}

        \item Если $\mathrm{cov}(\xi, \eta) > 0$, то зависимость прямая, если $\mathrm{cov}(\xi, \eta) < 0$, то обратная
    \end{enumerate}

    \hyperlink{correlation}{Коэффициентом корреляции} случайных величин $\xi$ и $\eta$ с конечными вторыми моментами,
    называется величина $r_{\xi,\eta} = \frac{\mathrm{cov(\xi, \eta)}}{\sqrt{D\xi} \sqrt{D\eta}} = \frac{E(\xi\eta) - E\xiE\eta}{\sigma_\xi \sigma_\eta}$
    
    Свойства:

    \begin{enumerate}
        \item $r_{\xi, \eta} = r_{\eta, \xi}$

        \item \begin{enumerate}
            \item Если $\xi$ и $\eta$ - независимы, то $r_{\xi,\eta} = 0$

            \item Если $r_{\xi,\eta} \neq 0$, то $\xi$ и $\eta$ - зависимы

            \item Если $r_{\xi,\eta} = 0$, то неясно
        \end{enumerate}

        \item $|r_{\xi,\eta}| \leq 1$

        \item $|r_{\xi,\eta}| = 1 \Longleftrightarrow \eta = a \xi + b$ п.н.

        \item \begin{enumerate} 
            \item Если $r_{\xi,\eta} = 1$, то $\eta = a\xi + b$ и $a > 0$ (прямая линейная зависимость)

            \item Если $r_{\xi,\eta} = -1$, то $\eta = a\xi + b$ и $a < 0$ (обратная линейная зависимость)
        \end{enumerate}
    \end{enumerate}

    Если $r_{\xi,\eta} \neq 0$, то говорят, что случайные величины коррелированы друг с другом. Если $r_{\xi,\eta} > 0$, 
    то имеет прямая корреляция, если $r_{\xi,\eta} < 0$ - обратная

    \item \textit{Характеристическая функция случайной величины, ее свойства. Теорема о непрерывном соответствии (формулировка).}

    \hyperlink{characteristicfunction}{Характеристической функций} случайной величины $\xi$ называется функция $\varphi_\xi(t) = Ee^{it\xi}, t \in \Real$
    
    Свойства:

    \begin{enumerate}
        \item Любая случайная величина $\xi$ имеет характеристическую функцию, причем $|\varphi_\xi(t)| \leq 1$

        \item Пусть $\varphi_\xi(t)$ - характеристическая функция случайной величины $\xi$. Тогда характеристическая функция
        случайной величины $a + b\xi$ равна $\varphi_{a + b\xi}(t) = e^{ita} \varphi_{\xi}(bt)$

        \item Характеристическая функция суммы независимых случайных величин равна произведению их характеристических функций

        \item Пусть $E\xi^k < \infty$. Тогда $\varphi_\xi(t) = 1 + it E\xi - \frac{t^2}{2}E\xi^2 + \dots + \frac{(it)^k}{k!} E\xi^k + o(|t|^k)$

        \item Пусть $E\xi^k < \infty$. Тогда $\varphi_\xi^{(k)}(0) = i^k E\xi^k$

        \item Существует взаимно-однозначное соответствие между распределениями и характеристическими функциями.
        Зная характеристическую функцию можно восстановить распределение.

        \item Теорема о непрерывном соответствии
        
        \Ths Последовательность случайных величин $\{\xi_n\}$ слабо сходится к $\xi$ тогда и только тогда, когда
            соответствующая последовательность характеристических функций сходится поточечно к $\varphi_\xi(t)$

        $\{\xi_n\} \rightrightarrows \xi \Longleftrightarrow \varphi_{\xi_n}(t) \longrightarrow \varphi_\xi(t) \forall t \in \Real$
    \end{enumerate}

    \item \textit{Характеристические функции стандартных распределений (Бернулли, биномиальное, Пуассона, нормальное). Следствия.}

    \hyperlink{characteristicfunctionofstandarddistributions}{Характеристические функции}: 

    \begin{itemize}
        \item Распределение Бернулли

        \begin{tabular}{c|c|c}
            $\xi$ & $0$     & $1$    \\
            \hline
            $p$   & $1 - p$ & $p$
        \end{tabular} \qquad\qquad $\varphi_\xi(t) = Ee^{i\xi t} = e^{i \cdot 0 \cdot t} p(\xi = 0) + e^{i \cdot 1 \cdot t} p(\xi = 1) = 1 - p + p e^{it}$

        \item Биномиальное распределение

        $P(\xi = k) = C_n^k p^k q^{n - k}, \quad k = 0, 1, \dots, n$

        Если $t \in B_{n,p}$, то $\xi = \xi_1 + \xi_2 + \xi_3 + \dots + \xi_n$, где $\xi_i \in B_p$ - независимы

        $\varphi_\xi(t) = (\varphi_{\xi_n}(t))^n = (1 - p + p e^{it})^n$

        \item Распределение Пуассона

        $P(\xi = k) = \frac{\lambda^k}{k!} e^{-\lambda}, \quad k = 0, 1, \dots, n$

        $\varphi_\xi(t) = e^{\lambda (e^{it} - 1)}$

        \underline{Следствие}: распределение Пуассона устойчиво относительно суммирования: $\letsymbol \xi \in \Pi_\lambda, \eta \in \Pi_\mu$, они независимы. Тогда $\xi + \eta \in \Pi_{\lambda + \mu}$

        \item Стандартное нормальное распределение

        $f_\xi(x) = \frac{1}{2\pi} e^{-\frac{x^2}{2}}$

        $\varphi_\xi(t) = e^{-\frac{t^2}{2}}$

        \item Нормальное распределение

        $\xi \in N(a, \sigma^2)$

        $\varphi_\xi(t) = e^{ita} \varphi_\eta(\sigma t) = e^{ita - \frac{\sigma^2t^2}{2}}$

        \underline{Следствие}: нормальное распределение устойчиво относительно суммирования: если $\xi \in N(a_1, \sigma_1^2), \eta \in N(a_2, \sigma^2_2)$ и они независимы, то $\xi + \eta \in N(a_1 + a_2, \sigma_1^2 + \sigma_2^2)$

    \end{itemize}

    \item \textit{Доказательство закона больших чисел Хинчина.}

    \hyperlink{lawofbignumberskhinchin2}{Закон больших чисел Хинчина}
    
    Пусть $\xi_1, \xi_2, \dots, \xi_n$ - последовательность независимых одинаково распределенных случайных величин с конечным матожиданием.
    Тогда $\frac{S_n}{n} = \frac{\xi_1 + \dots + \xi_n}{n} \overset{p}{\longrightarrow} E\xi_1$
    
    \begin{MyProof}
        Обозначим $a = E\xi_1$
    
        Ранее было доказано, что сходимость по вероятности к константе эквивалентно к слабой сходимости. Поэтому достаточно доказать, что $\frac{S_n}{n} \rightrightarrows a$
    
        По теореме о непрерывном соответствии остается доказать, что $\varphi_{\frac{S_n}{n}}(t) \longrightarrow \varphi_a(t) = e^{ita}$
    
        По четвертому свойству $\varphi_{\xi_1}(t) = 1 + itE\xi_1 + o(|t|) = 1 + ita + o(|t|)$
    
        $\varphi_{\frac{S_n}{n}}(t) = [\text{по второму свойству}] = \varphi_{S_n}\left(\frac{t}{n}\right) = \left(\varphi_{\xi_1}\left(\frac{t}{n}\right)\right)^n = \left(1 + ia\frac{t}{n} + o\left(\left|\frac{t}{n}\right|\right)\right)^n \underset{\text{по лемме}}{\longrightarrow}
        e^{ita} = \varphi_a(t)$
    \end{MyProof}

    \item \textit{Центральная предельная теорема. Вывод из нее предельной теоремы Муавра-Лапласа. Неравенство Берри-Ессеена (формулировка). }

    \hyperlink{centrallimittheorem}{ЦПТ Ляпунова} \Ths Пусть $\xi_1, \dots, \xi_n, \dots$ - последовательность независимых одинаково распределенных случайных величин
    с конечной дисперсией ($D\xi_1 < \infty$) и $S_n = \sum_{i = 1}^n \xi_i$. Тогда имеет место слабая сходимость:

    \[\frac{S_n - nE\xi_1}{\sqrt{nD\xi_1}} \rightrightarrows N(0, 1)\]

    \hyperlink{limittheoremdemoivrelaplace}{Предельная теорема Муавра-Лапласа}: Пусть $v_n(A)$ - число появления события $A$ при $n$ независимых испытаний, $p$ - вероятность успеха при одном испытании, $q = 1 - p$.
    Тогда $\frac{v_n(A) - np}{\sqrt{npq}} \rightrightarrows N(0, 1)$

    \begin{MyProof}
        $v_n(A) = \xi_1 + \xi_2 + \dots + \xi_n = S_n$, где $\xi_i \in B_p$ и независимы, $E\xi_1 = p, D\xi_1 = pq$

        По ЦПТ $\frac{v_n(A) - np}{\sqrt{npq}} = \frac{S_n - nE\xi_1}{\sqrt{nD\xi_1}} \rightrightarrows N(0, 1)$
    \end{MyProof}

    \hyperlink{berryesseentheorem}{Неравенство Берри-Эссеена}: В условиях ЦПТ для $\xi_1$ с конечным третьим моментом можно оценить так:

    $\left|p\left(\frac{S_n - nE\xi_1}{\sqrt{nD\xi_1}} < x\right) - F_0(x)\right| \leq C\frac{E|\xi_1 - E\xi_1|^3}{\sqrt{n(D\xi_1)^3}} \quad \forall x \in \Real$


\end{enumerate}
