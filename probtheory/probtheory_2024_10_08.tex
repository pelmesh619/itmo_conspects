\documentclass[12pt]{article}
\usepackage{preamble}

\pagestyle{fancy}
\fancyhead[LO,LE]{Теория вероятности}
\fancyhead[CO,CE]{08.10.2024}
\fancyhead[RO,RE]{Лекции Блаженова А. В.}

\fancyfoot[L]{\scriptsize исходники найдутся тут: \\ \url{https://github.com/pelmesh619/itmo_conspects} \Cat}

\begin{document}
    \section{Лекция 6}

    \subsection{Случайные величины}

    Примеры случайных величин:

    \ExN{1} Бросаем кость, может выпасть 6 граней, здесь случайная величина $\xi$ - число выпавших очков

    \ExNs{2} $\xi$ - время работы микросхемы, в этом случае время может быть:

    а) дискретным - $\xi \in \{0, 1, 2, \dots\}$

    б) непрерывным - $\xi \in [0; \infty)$

    \ExNs{3} Температура за окном: $\xi \in (-50, +50)$

    \hypertarget{measurabilityoffunction}{}

    \Def На вероятностном пространстве $(\Omega, \mathcal{F}, p)$ функция $\xi \ : \ \Omega \to \Real$ называется
    $\mathcal{F}$-измеримой, если $\forall x \in \Real \ \{\omega \in \Omega \ | \ \xi(\omega) < x\} \in \mathcal{F}$
    (то есть $\xi^{-1}(y) \in \mathcal{F}$, где $y \in (-\infty; x)$)

    \hypertarget{randomvaluedefinition}{}

    \Def Случайной величиной, заданной на вероятностном пространстве $(\Omega, \mathcal{F}, p)$, называется
    $\mathcal{F}$-измеримая функция $\xi \ : \Omega \to \Real$, которая сопоставляет каждому элементарному исходу \
    некоторое вещественное число

    \Nota Не все функции являются $\mathcal{F}$-измеримыми

    \Exs Кость: $\Omega = \{1, 2, 3, 4, 5, 6\}; \mathcal{F} = \{\emptyset, \Omega, \{2, 4, 6\}, \{1, 3, 5\}\}$

    Пусть $\xi(\omega) = i$ - число выпавших очков. Тогда при $x = 4: \{\omega \in \Omega \ | \ \xi (\omega) < 4\} = \{1, 2, 3\} \notin \mathcal{F} \Longrightarrow$ случайная величина не является $\mathcal{F}$-измеримой

    В данном случае следует сделать $\xi$ таким, что $\xi(2) = \xi(4) = \xi(6) = 1$, $\xi(1) = \xi(3) = \xi(5) = 0$

    \Nota Смысл измеримости: если задана случайная величина $\xi$, то мы можем задать вероятность попадания случайной
    величины в интервал $(-\infty; x)$: $p(\xi \in (-\infty; x)) = p(\{\omega \in \Omega \ | \ \xi(\omega) < x\})$

    А из интервалов $(-\infty; x)$ с помощью операций пересечения, объединения и дополнения можно получить все другие
    интервалы (включая точки) и также приписать им вероятности

    Из матанализа известно, что мера из интервалов однозначно продолжается до меры на всей Борелевской $\sigma$-алгебры на $\Real$
    и, таким образом, с помощью случайной величины каждому Борелевскому множеству $B$ также приписывается вероятность $p(\xi \in B)$

    \hypertarget{probabilityspacerbp}{}

    Итак, пусть $\xi$ задана на вероятностном пространстве $(\Omega, \mathcal{F}, p)$, с помощью нее получаем новой вероятностное
    пространство $(\Real, \mathcal{B}(\Real), p_\xi)$

    Получая новое вероятностное пространство, мы упрощаем и формализуем работу, так как можем не учитывать природу и структуру исходного пространства

    \hypertarget{randomvaluedistribution}{}

    \Def Функция $p(B), B \in \mathcal{B}(\Real)$, ставящая в соответствие каждому Борелевскому множеству вероятность,
    называется распределением случайной величины $\xi$

    \subsection{Основные типы распределения}

    \begin{enumerate}[label=\alph*) ]
        \item Дискретное

        \item Абсолютно непрерывное

        \item Сингулярное

        \item Смешанное
    \end{enumerate}

    \subsection{Дискретная случайная величина}

    \hypertarget{discreterandomvalue}{}

    \Def Случайная величина $\xi$ имеет дискретное рапределение, если она принимает не более, чем счетное число значений.
    То есть существует конечный или счетный набор чисел $\{x_1, x_2, \dots, x_n, \dots\}$ такой, что $p(\xi = x_i) = p_i > 0$ и $\sum_{i = 0}^\infty p_i = 1$

    Таким образом, дискретная случайная величина (ДСВ) задается законом распределения:

    \smallvspace

    \begin{tabular}{c|c|c|c|c|cl}
        $\xi$ & $x_1$ & $x_1$ & \dots & $x_n$ & \dots & \text{\qquad   - значения случайной величины} \\
        \cline{1-6}
        $p$   & $p_1$ & $p_1$ & \dots & $p_n$ & \dots & \text{\qquad   - вероятности этих значений}
    \end{tabular}

    \smallvspace


    ($\sum_{i = 0}^\infty p_i = 1$ - условие нормировки)

    \ExN{1} кость, $\xi(\omega) = i$ - число выпавших очков

    \smallvspace

    %nodisplay

    \begin{tabular}{c|c|c|c|c|c|c}
        $\xi$ & $1$           & $2$           & $3$           & $4$           & $5$           & $6$           \\
        \hline
        $p$   & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$
    \end{tabular}

    %yesdisplay

    \smallvspace

    \ExNs{2} все распределения из предыдущих лекций (биномиальное, геометрическое, гипергеометрическое, Пуассона)

    \ExNs{3} индикатор события $A$: $I_A (\omega) = \begin{cases}
                                                        0, \quad \omega \notin A \text{ - событие } A \text{ не происходит} \\ 1, \quad \omega \in A \text{ - событие } A \text{ происходит}
    \end{cases}$

    \hypertarget{attributesofdiscreterandomvalue}{}

    \subsection{Числовые характеристики дискретных случайных величин}

    \subsubsection{I. Математическое ожидание (среднее значение, полезность)}

    \Defs Математическим ожиданием $E\xi$ случайной величины $\xi$ называется число

    \[ E\xi = \sum_{i = 1}^\infty x_i p_i \]

    при условии, что данный ряд сходится абсолютно

    \Nota Если $E\xi = \sum_{i = 1}^\infty x_i p_i = \infty$, то говорят, что матожидание не существует

    При условной сходимости ряда при перестановке членов сумма изменяется, поэтому необходима абсолютная

    \textbf{Физический смысл}: Среднее значение - число, вокруг которого группируются значения случайной величины, центр тяжести точек $x_i$ с весами $p_i$

    \textbf{Статистический смысл}: среднее арифметическое наблюдаемых значений случайной величины при
    большом числе реальных экспериментов

    \subsubsection{II. Дисперсия}

    \Defs Дисперсией $D\xi$ случайной величины $\xi$ называют среднее квадратов ее отклонения от математического ожидания:

    $D\xi = E (\xi - E\xi)^2$ или $D\xi = \sum_{i = 0}^\infty (x_i - E\xi)^2 p_i$ при условии, что данный ряд сходится

    В противном случае говорится, что дисперсии не существует

    \Nota Дисперсию обычно удобно считать по формуле $D\xi = E\xi^2 - (E\xi)^2 = \sum_{i = 1}^n x^2_i p_i - E\xi^2$

    \textbf{Смысл} - квадрат среднего разброса (рассеивания) значения случайной величины относительно ее математического
    ожидания

    \subsubsection{III. Среднее квадратическое отклонение}

    \Defs Средним квадратическим отклонением (СКО) $\sigma_\xi$ называется величина $\sigma_\xi = \sqrt{D\xi}$

    \textbf{Смысл} - средний разброс

    \ExN{1} Кость

    \smallvspace

    %nodisplay

    \begin{tabular}{c|c|c|c|c|c|c}
        $\xi$ & $1$           & $2$           & $3$           & $4$           & $5$           & $6$           \\
        \hline
        $p$   & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$ & $\frac{1}{6}$
    \end{tabular}

    %yesdisplay

    $E\xi = \sum_{i = 1}^6 x_i p_i = 3.5$ (в данном случае ср. арифм.)

    $D\xi = \sum_{i = 1}^6 (x_i - E\xi)^2 p_i = 1^2 \cdot \frac{1}{6} + 2^2 \cdot \frac{1}{6} + 3^2 \cdot \frac{1}{6} + 4^2 \cdot \frac{1}{6} + 5^2 \cdot \frac{1}{6} + 6^2 \cdot \frac{1}{6} - 3.5^2 = \frac{35}{12} $

    $\sigma_\xi = \sqrt{D\xi} \approx 1.79$

    \ExN{2} Индикатор события $A$: $I_A (\omega) = \begin{cases}
                                                       0, \omega \notin A \text{ - событие } A \text{ не происходит} \\ 1, \omega \in A \text{ - событие } A \text{ происходит}
    \end{cases}$

    \begin{tabular}{c|c|c}
        $\xi$ & $0$        & $1$    \\
        \hline
        $p$   & $1 - P(A)$ & $P(A)$
    \end{tabular}

    $E\xi = 0 \cdot (1 - P(A)) + 1 \cdot P(A) = P(A)$

    $D\xi = 0^2 \cdot (1 - P(A)) + 1^2 P(A) - P(A)^2 = P(A) (1 - P(A)) = pq$

    $\sigma_\xi = \sqrt{pq}$

    \hypertarget{expectedvalueandvarianceproperties}{}

    \subsection{Свойства матожидания и дисперсии}

    \begin{MyTheorem}
        \ThNs{1} Случайная величина $\xi$ имеет вырожденное распределение, если $\xi(\omega) = \mathrm{const} \ \ \forall \omega \in \Omega$

        \begin{tabular}{c|c}
            $\xi$ & $C$ \\
            \hline
            $p$   & $1$
        \end{tabular}

        $E\xi = C \qquad D\xi = 0$
    \end{MyTheorem}

    \begin{MyTheorem}
        \ThNs{2} Свойство сдвига: $E(\xi + C) = E\xi + C; D (\xi + C) = D\xi$
    \end{MyTheorem}

    \begin{MyTheorem}
        \ThNs{3} Свойство растяжения:

        $E(C\xi) = CE\xi$

        $D(C\xi) = C^2 D\xi$
    \end{MyTheorem}

    \Lab 2-3 доказать

    \begin{MyTheorem}
        \ThNs{4} $E(\xi + \eta) = E\xi + E\eta$ (из третьего свойства матожидание - линейная функция)
    \end{MyTheorem}

    \begin{MyProof}
        $\Box$

        $\letsymbol x_i, y_i$ - значения случайных величин $\xi, \eta$, а $p_i$ и $q_i$ - их соответствующие вероятности

        $E(\xi + \eta) = \sum_{i, j} (x_i + y_j) p(\xi = x_i \text{ и } \eta = y_j) = \sum_i x_i \sum_j p(\xi = x_i \text{ и } \eta = y_j) + \sum_j y_j \sum_i p(\xi = x_i \text{ и } \eta = y_j) =
        \sum_i x_i p(\xi = x_i) + \sum_j y_j p(\eta = y_j) = E\xi + E\eta$

        $\Box$
    \end{MyProof}

    \Def Дискретные случайные величины $\xi$ и $\eta$ независимы, если $p(\xi = x_i, \eta = y_i) = p(\xi = x_i) \cdot p(\eta = y_i) \ \forall i, j$

    То есть случайные величины принимают свои величины независимо друг от друга

    \begin{MyTheorem}
        \ThNs{5} Если случайные величины $\xi$ и $\eta$ независимы, то $E(\xi \eta) = E\xi \cdot E\eta$; обратное неверно
    \end{MyTheorem}

    \begin{MyProof}
        $\Box$

        $E(\xi\eta) = \sum_{i, j} x_i y_i p(\xi = x_i, \eta = y_j) = \sum_i x_i \sum_j y_j p(\xi = x_i, \eta = y_j) =
        \sum_i x_i \sum_j y_j p(\xi = x_i) p(\eta = y_j) = \sum_i x_i p(\xi = x_i) \sum_j y_j p(\eta = y_j) = E\xi \cdot E\eta$

        $\Box$
    \end{MyProof}

    \begin{MyTheorem}
        \ThNs{6} $D\xi = E\xi^2 - (E\xi)^2$
    \end{MyTheorem}

    \begin{MyProof}
        $\Box$

        $D\xi = E(\xi - E\xi)^2 = E(\xi^2 - 2\xi E\xi + (E\xi)^2) = E\xi^2 - 2E\xi E\xi + E((E\xi)^2) =
        E\xi^2 - 2(E\xi)^2 + (E\xi)^2 = E\xi^2 - (E\xi)^2$

        $\Box$
    \end{MyProof}

    \Def $D(\xi + \eta) = D\xi + D\eta + 2\mathrm{cov} (\xi, \eta)$,
    где $\mathrm{cov}(\xi, \eta) = E(\xi\eta) - E\xi E\eta$ - ковариация случайных величин (равна 0 при независимых величинах) - индикатор наличия связи между случайными величинами

    \begin{MyProof}
        $\Box$

    $D(\xi + \eta) = E(\xi + \eta)^2 - (E(\xi + \eta))^2 = E\xi^2 + 2E(\xi \eta) + E\eta^2 - (E\xi + E\eta)^2 =
    E\xi^2 + E\eta^2 + 2E(\xi\eta) - (E\xi)^2 - (E\eta)^2 - 2E\xi E\eta = D\xi + D\eta + 2\mathrm{cov}(\xi, \eta)$

        $\Box$
    \end{MyProof}


    \begin{MyTheorem}
        \ThNs{7} Если случайные величины $\xi$ и $\eta$ независимы, то $D(\xi + \eta) = D\xi + D\eta$
    \end{MyTheorem}

    \begin{MyProof}
        $\Box$

        Если $\xi$ и $\eta$ независимы, то $\mathrm{cov}(\xi, \eta) = 0$ и $D(\xi + \eta) = D\xi + D\eta$

        $\Box$
    \end{MyProof}

    \begin{MyTheorem}
        \ThNs{8} Общая формула дисперсии суммы: $D(\xi_1 + \xi_2 + \dots + \xi_n) = \sum_{i = 1}^n D \xi_i + 2\sum_{i, j (i \neq j)} \mathrm{cov} (\xi_i, \xi_j)$
    \end{MyTheorem}

    \subsection{Другие числовые характеристики}

    Моменты старших порядков

    а) $m_k = E\xi^k$ - момент $k$-ого порядка случайной величины $\xi$ (также называют начальным моментом)

    б) $\mu_k = E(\xi - E\xi)^k$ - центральный момент $k$-ого порядка

    $E\xi = m_1$ - момент первого порядка

    $E\xi^2 = m_2$ - момент второго порядка

    $D\xi = E(\xi - E\xi)^2$ - центральный момент второго порядка

    \Nota Центральные моменты можно выразить через обычный момент:

    $\mu_2 = D\xi = E\xi^2 - (E\xi)^2 = m_2^2 - m_1^2$

    $\mu_3 = m_3 - 3m_2 m_1 + 2m^3$

    $\mu_4 = m_4 - 4m_3 m_2 + 6m_2 m_1^2 - 3m_1^4$

    \Ex Разберем \hyperlink{buffonsproblem}{задачу Бюффона} с точки зрения матожидания (для простоты $l$ - ширина доски): пусть $p(A)$ - пересечет стык,
    $\xi = I_A$ - число пересечений. Тогда матожидание $E\xi = E I_A = P(A)$

    Заметим, что при изменении длины иглы с $l$ до $2l$ матожидание пересекаемых стыков увеличивается
    в два раза. Помимо этого можно составить из $k$ игл ломаную, матожидание стыков которой будет равно $kE\xi$

    Заметим, что такое работает и в обратную сторону: при уменьшении иглы в $k$ раз матожидание равно $\frac{E\xi}{k}$

    Теперь сделаем замкнутый многоугольник из игл, получим, что матожидание в таком случае $P\frac{E\xi}{l}$, где $P$ - периметр

    В пределе строим круг диаметра $l$ - он всегда пересечет линии стыка 2 раза, значит матожидание $E_o = P_o\frac{E\xi}{l} = 2$

    Длина окружность $P_o = \pi l$, получаем $E\xi = \frac{2l}{P_o} = \frac{2l}{\pi l} = \frac{2}{\pi}$

\end{document}
