$subject$=Теория вероятности
$date$=17.12.2024
$teacher$=Лекции Блаженова А. В.

\section{Лекция 16}

\subsection{Условная дисперсия}

\Def Условной дисперсией случайной величины $\xi$ относительно случайной величины $\eta$ называется случайная величина 
$D(\xi | \eta) = E((\xi - E(\xi | \eta))^2 | \eta)$

\Nota То есть дисперсия соответствующего условного распределения

\underline{Свойства}

\begin{enumerate}
    \item $D(\xi | \eta) = E(\xi^2 | \eta) - E^2(\xi | \eta)$

    \item Закон полной дисперсии

    \begin{MyTheorem}
        \Ths $D\xi = E(D(\xi | \eta)) + D(E(\xi | \eta))$
    \end{MyTheorem}

    \begin{MyProof}
        Из первого свойства $E(\xi^2 | \eta) = D(\xi | \eta) + E^2(\xi | \eta)$

        $D\xi = E\xi^2 - (E\xi)^2 = E(E\xi^2 | \eta) - E^2(E(\xi | \eta)) = E(D(\xi | \eta) + E^2(\xi | \eta)) - E^2(E(\xo | \eta)) = E(D(\xi | \eta)) + E(E^2(\xi | \eta)) - E^2(E(\xi | \eta)) = E(D(\xi | \eta)) + D(E(\xi | \eta))$
    \end{MyProof}

    \underline{Следствие и смысл}: 
    
    \begin{itemize}
        \item Если $\xi$ и $\eta$ независимы (некоррелированы), то $D(E(\xi | \eta)) = D(E\xi) = 0$ и $D\xi = E(D(\xi | \eta))$

        \item Если имеется функциональная зависимость (то есть $\xi = g(\eta)$), то $D(E(\xi | \eta)) = D(E(g(\eta) | \eta)) = 
        D(g(\eta)) = D\xi$

        \item Таким образом по величине $R^2 = \frac{D(E(\xi | \eta))}{D\xi}$ ($0 \leq R^2 \leq 1$) можно судить о силе корреляционной зависимости.
        Такая величина называется корреляционным отношением
    \end{itemize}
\end{enumerate}

\subsection{Энтропия}

Пусть $\xi$ - результат эксперимента с исходами $A_1, A_2, \dots, A_N$, вероятности которых $p_1, p_2, \dots, p_N$

\Def Энтропией эксперимента называется величина $H(\xi) = -\sum_{i = 1}^N p_i \cdot \log_2 p_i$

\underline{Свойства энтропии}:

\begin{enumerate}
    \item Очевидно, что $H(\xi) \geq 0$, так как $p \geq 0$, а $\log_2 p_i \leq 0$
    
    \item $H(\xi) = 0 \Longleftrightarrow \exists i$, такой что $p_i = 1, p_j = 0 \forall j \neq i$ - то есть эксперимент заканчивается всегда одним исходом, нет неопределенности

    \item Максимум $H(\xi) = \log_2 N = H_0$ достигается при $p_1 = p_2 = \dots = \frac{1}{N}$ - то есть когда все вероятности одинаковы, ни одному исходу нельзя отдать предпочтение, и результат эксперимента получается максимально неопределенным

    \begin{MyProof}
        Рассмотрим $\varphi(x) = x \log_2 x$. Так как $\varphi^{\prime\prime}(x) = \frac{1}{x\ln 2} > 0$ при $x > 0$, следовательно $\varphi(x)$ выпукла вниз

        Рассмотрим случайную величину $\eta$

        \begin{tabular}{c|c|c|c|c}
            $\eta$ & $p_1$ & $p_2$ & \dots & $p_n$ \\
            \cline{1-6}
            $p$   & $\frac{1}{N}$ & $\frac{1}{N}$ & \dots & $\frac{1}{N}$
        \end{tabular}

        По неравенству Йенсена $\varphi(E\eta) = \varphi(\sum_{i = 1}^N \frac{p_i}{N}) = \varphi(\frac{1}{N} \sum_{i = 1}^N p_i) = 
        \varphi(\frac{1}{N}) = \frac{\log_2 \frac{1}{N}}{N} \leq E(\varphi(\eta)) = 
        \frac{1}{N} \sum_{i = 1}^N p_i \log_2 p_i = -\frac{1}{N} H(\eta)$

        Получаем $\frac{\log_2 \frac{1}{N}}{N} \leq -\frac{1}{N} H(\eta)$, то есть $H \leq \log_2 N$
    \end{MyProof}

    \underline{Следствие}: Энтропию можно рассматривать как меру неопределенности эксперимента
\end{enumerate}

\Ex $\xi \in B_{p}$

\begin{tabular}{c|c|c}
    $\xi$ & $0$ & $1$ \\
    \cline{1-6}
    $p$   & $1 - p$ & $p$
\end{tabular}

$H(\xi) = -(1 - p) \log_2 (1 - p) - p \log_2 p$

% график

\ExN{1} Психолог Р. Хайман проводил такой эксперимент: перед человеком загорались с некоторой частотой лампочки, замерялась время реакции на загоревшуюся лампочку. 
Если лампочки загорались с одинаковой частотой, то энтропия была пропорциональна $H_0$

\ExN{2} Также с помощью энтропии определен второй закон термодинамики

\ExN{3} Теория кодирования информации

Если алфавит сообщения состоит из $N$ символов, то каждому символу присваиваем последовательность одинаковой длины из 0 и 1, 
причем ее длина будет $\lceil\log_2 N\rceil$

Для передачи $n$ символов потребуется последовательность длиной $n\lceil\log_2 N\rceil$

Цель: сократить длину последовательности

Для больших по объему сообщений можно заметно уменьшить эту величину, используя, что разные символы встречаются с разными частотами.

Если $p_1, p_2, \dots, p_N$ - эти частоты, то в сообщении длиной $N$ $i$-ый символ появляется $v_i \approx n p_i$ раз 

\Def Сообщение длины $N$ называется типичным с параметрами $n$ и $\delta$, если $|v_i - n p_i| < \delta \ \forall 1 \leq i \leq N$

Пусть $M_{n, \delta}$ - число таких сообщений

\begin{MyTheorem}
    \Ths (частный случай теоремы Макмиллана)

    $\frac{1}{n} \log_2 M_{n, \delta} \underset{n \to \infty}{\longrightarrow} H = -\sum_{i = 1}^N p_i \log_2 p_i$
\end{MyTheorem}

\underline{Следствие}: существует $\varepsilon > 0 \ | \ \frac{1}{n} \log_2 M_{n,\delta} < H + \varepsilon$ (или $M_{n, \delta} < 2^{n(H + \varepsilon)}$)

Если можно занумеровать эти типичные сообщения, то для них потребуется число символов $\log_2 2^{n(H + \varepsilon)} = n \cdot (H + \varepsilon)$

И поэтому с вероятностью приблизительно 1 можно сократить длины сообщение с коэффициентом сжатия $\gamma \approx \frac{nH}{nH_0} = \frac{H}{H_0}$, где $H_0 = \log_2 N$

Если все символы встречаются независимо, то дальнейшее сжатие невозможно, но так как буквы встречаются в определенных сочетаниях, то можно сжать 
информации дальше, используя этот факт

Пусть $\gamma_\infty$ - коэффициент итогового сжатия

% здесь не понял, что он точно сказал

В русском языке $\gamma \approx 0.87$. Если считать слова символами нашего алфавита, 
то получится $\gamma_\infty \approx 0.24$ для литературного языка и
$\gamma_\infty \approx 0.17$ для делового языка

\Def $1 - \gamma_\infty$ называют коэффициентом избыточности языка

\subsection{Энтропия при непрерывном распределении}

\Def Пусть $\xi$ абсолютно непрерывная случайная величина с плотностью $f(x)$ и носителем $A = \{x \ | \ f(x) > 0\}$. Энтропией $H(\xi)$ называется
величина $-\int_A f(x) \log_2 f(x) dx$

\begin{MyTheorem}
    \Ths Следующие распределения имеют наибольшую энтропию:

    \begin{enumerate}
        \item Если $A = [0, 1]$, то $U(0, 1)$

        \item Если $A = [0, \infty)$ и $E\xi = 1$, то показательное $E_1$

        \item Если $A = \Real$ и $E\xi = 0$, а $D\xi = 1$, то $N(0, 1)$
    \end{enumerate}
\end{MyTheorem}

